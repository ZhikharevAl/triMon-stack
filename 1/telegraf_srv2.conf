# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active. To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config file would generate.
#
# Environment variables can be used anywhere in this config file, simply surround them with ${}. For strings the variable must be within quotes (ie, "${STR_VAR}"), for numbers and booleans they 
# should be plain (ie, ${INT_VAR}, ${BOOL_VAR}) Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1 rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"
# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "30s"
  ## Rounds collection interval to 'interval' ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true
  ## Telegraf will send metrics to outputs in batches of at most metric_batch_size metrics. This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000
  ## Maximum number of unwritten metrics per output.  Increasing this value allows for longer periods of output downtime without dropping metrics at the cost of higher maximum memory usage.
  metric_buffer_limit = 10000
  ## Collection jitter is used to jitter the collection by a random amount. Each plugin will sleep for a random time within jitter before collecting. This can be used to avoid many plugins querying 
  ## things like sysfs at the same time, which can have a measurable effect on the system.
  collection_jitter = "0s"
  ## Collection offset is used to shift the collection by the given amount. This can be be used to avoid many plugins querying constraint devices at the same time by manually scheduling them in 
  ## time.
  # collection_offset = "0s"
  ## Default flushing interval for all outputs. Maximum flush_interval will be flush_interval + flush_jitter
  flush_interval = "30s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid large write spikes for users running a large number of telegraf instances. ie, a jitter of 5s and interval 10s means 
  ## flushes will happen every 10-15s
  flush_jitter = "0s"
  ## Collected metrics are rounded to the precision specified. Precision is specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s). Valid time units are "ns", "us" (or "Âµs"), "ms", 
  ## "s".
  ##
  ## By default or when set to "0s", precision will be set to the same timestamp order as the collection interval, with the maximum being 1s:
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ##
  ## Precision will NOT be used for service inputs. It is up to each individual service input to set the timestamp at the appropriate precision.
  precision = "0s"
  ## Log at debug level.
  # debug = false
  ## Log only error level messages.
  # quiet = false
  ## Log target controls the destination for logs and can be one of "file", "stderr" or, on Windows, "eventlog".  When set to "file", the output file is determined by the "logfile" setting.
  # logtarget = "file"
  ## Name of the file to be logged to when using the "file" logtarget.  If set to the empty string then logs are written to stderr.
  # logfile = ""
  ## The logfile will be rotated after the time interval specified.  When set to 0 no time based rotation is performed.  Logs are rotated only when written to, if there is no log activity rotation 
  ## may be delayed.
  # logfile_rotation_interval = "0h"
  ## The logfile will be rotated when it becomes larger than the specified size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"
  ## Maximum number of rotated archives to keep, any older logs are deleted. If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5
  ## Pick a timezone to use when logging or type 'local' for local time. Example: America/Chicago
  # log_with_timezone = ""
  ## Override default hostname, if empty use os.Hostname()
  hostname = "centos-monitor"
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false
  ## Method of translating SNMP objects. Can be "netsnmp" (deprecated) which translates by calling external programs snmptranslate and snmptable, or "gosmi" which translates using the built-in gosmi 
  ## library.
  # snmp_translator = "netsnmp"
  ## Name of the file to load the state of plugins from and store the state to. If uncommented and not empty, this file will be used to save the state of stateful plugins on termination of Telegraf. 
  ## If the file exists on start, the state in the file will be restored for the plugins.
  # statefile = ""
###############################################################################
#                            SECRETSTORE PLUGINS #
###############################################################################
# # File based Javascript Object Signing and Encryption based secret-store
# [[secretstores.jose]]
#   ## Unique identifier for the secret-store. This id can later be used in plugins to reference the secrets in this secret-store via @{<id>:<secret_key>} (mandatory)
#   id = "secretstore"
#
#   ## Directory for storing the secrets
#   path = "/etc/telegraf/secrets"
#
#   ## Password to access the secrets. If no password is specified here, Telegraf will prompt for it at startup time.
#   # password = ""
# # Operating System native secret-store
# [[secretstores.os]]
#   ## Unique identifier for the secret-store. This id can later be used in plugins to reference the secrets in this secret-store via @{<id>:<secret_key>} (mandatory)
#   id = "secretstore"
#
#   ## Keyring Name & Collection * Linux: keyring name used for the secrets, collection is unused * macOS: keyring specifies the macOS' Keychain name and collection is an
#   ##     optional Keychain service name * Windows: keys follow a fixed pattern in the form `<keyring>:<collection>:<key>`. Please keep this in mind when creating secrets with the Windows 
#   ##     credential tool.
#   # keyring = "telegraf" collection = ""
#
#   ## macOS Keychain password If no password is specified here, Telegraf will prompt for it at startup time.
#   # password = ""
#
#   ## Allow dynamic secrets that are updated during runtime of telegraf
#   # dynamic = false
###############################################################################
#                            OUTPUT PLUGINS #
###############################################################################
# # Configuration for sending metrics to InfluxDB 2.0
# [[outputs.influxdb_v2]]
#   ## The URLs of the InfluxDB cluster nodes.
#   ##
#   ## Multiple URLs can be specified for a single cluster, only ONE of the urls will be written to each interval.
#   ##   ex: urls = ["https://us-west-2-1.aws.cloud2.influxdata.com"]
#   urls = ["http://127.0.0.1:8086"]
#
#   ## Token for authentication.
#   token = ""
#
#   ## Organization is the name of the organization you wish to write to.
#   organization = ""
#
#   ## Destination bucket to write into.
#   bucket = ""
#
#   ## The value of this tag will be used to determine the bucket.  If this tag is not set the 'bucket' option is used as the default.
#   # bucket_tag = ""
#
#   ## If true, the bucket tag will not be added to the metric.
#   # exclude_bucket_tag = false
#
#   ## Timeout for HTTP messages.
#   # timeout = "5s"
#
#   ## Additional HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## HTTP Proxy override, if unset values the standard proxy environment variables are consulted to determine which proxy, if any, should be used.
#   # http_proxy = "http://corporate.proxy:3128"
#
#   ## HTTP User-Agent
#   # user_agent = "telegraf"
#
#   ## Content-Encoding for write request body, can be set to "gzip" to compress body or "identity" to apply no encoding.
#   # content_encoding = "gzip"
#
#   ## Enable or disable uint support for writing uints influxdb 2.0.
#   # influx_uint_support = false
#
#   ## Optional TLS Config for use on HTTP connections.
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Configuration for Amon Server to send metrics to.
# [[outputs.amon]]
#   ## Amon Server Key
#   server_key = "my-server-key" # required.
#
#   ## Amon Instance URL
#   amon_instance = "https://youramoninstance" # required
#
#   ## Connection timeout.
#   # timeout = "5s"
# # Publishes metrics to an AMQP broker
# [[outputs.amqp]]
#   ## Broker to publish to.
#   ##   deprecated in 1.7; use the brokers option
#   # url = "amqp://localhost:5672/influxdb"
#
#   ## Brokers to publish to.  If multiple brokers are specified a random broker will be selected anytime a connection is established.  This can be helpful for load balancing when not using a 
#   ## dedicated load balancer.
#   brokers = ["amqp://localhost:5672/influxdb"]
#
#   ## Maximum messages to send over a connection.  Once this is reached, the connection is closed and a new connection is made.  This can be helpful for load balancing when not using a dedicated 
#   ## load balancer.
#   # max_messages = 0
#
#   ## Exchange to declare and publish to.
#   exchange = "telegraf"
#
#   ## Exchange type; common types are "direct", "fanout", "topic", "header", "x-consistent-hash".
#   # exchange_type = "topic"
#
#   ## If true, exchange will be passively declared.
#   # exchange_passive = false
#
#   ## Exchange durability can be either "transient" or "durable".
#   # exchange_durability = "durable"
#
#   ## Additional exchange arguments.
#   # exchange_arguments = { } exchange_arguments = {"hash_property" = "timestamp"}
#
#   ## Authentication credentials for the PLAIN auth_method.
#   # username = "" password = ""
#
#   ## Auth method. PLAIN and EXTERNAL are supported Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as described here: https://www.rabbitmq.com/plugins.html
#   # auth_method = "PLAIN"
#
#   ## Metric tag to use as a routing key.
#   ##   ie, if this tag exists, its value will be used as the routing key
#   # routing_tag = "host"
#
#   ## Static routing key.  Used when no routing_tag is set or as a fallback when the tag specified in routing tag is not found.
#   # routing_key = "" routing_key = "telegraf"
#
#   ## Delivery Mode controls if a published message is persistent.
#   ##   One of "transient" or "persistent".
#   # delivery_mode = "transient"
#
#   ## InfluxDB database added as a message header.
#   ##   deprecated in 1.7; use the headers option
#   # database = "telegraf"
#
#   ## InfluxDB retention policy added as a message header
#   ##   deprecated in 1.7; use the headers option
#   # retention_policy = "default"
#
#   ## Static headers added to each published message.
#   # headers = { } headers = {"database" = "telegraf", "retention_policy" = "default"}
#
#   ## Connection timeout.  If not provided, will default to 5s.  0s means no timeout (not recommended).
#   # timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional Proxy Configuration
#   # use_proxy = false proxy_url = "localhost:8888"
#
#   ## If true use batch serialization format instead of line based delimiting. Only applies to data formats which are not line based such as JSON. Recommended to set to true.
#   # use_batch_format = false
#
#   ## Content encoding for message payloads, can be set to "gzip" to or "identity" to apply no encoding.
#   ##
#   ## Please note that when use_batch_format = false each amqp message contains only a single metric, it is recommended to use compression with batch format for best results.
#   # content_encoding = "identity"
#
#   ## Data format to output. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   # data_format = "influx"
# # Send metrics to Azure Application Insights
# [[outputs.application_insights]]
#   ## Instrumentation key of the Application Insights resource.
#   instrumentation_key = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx"
#
#   ## Regions that require endpoint modification https://docs.microsoft.com/en-us/azure/azure-monitor/app/custom-endpoints
#   # endpoint_url = "https://dc.services.visualstudio.com/v2/track"
#
#   ## Timeout for closing (default: 5s).
#   # timeout = "5s"
#
#   ## Enable additional diagnostic logging.
#   # enable_diagnostic_logging = false
#
#   ## Context Tag Sources add Application Insights context tags to a tag value.
#   ##
#   ## For list of allowed context tag keys see: https://github.com/microsoft/ApplicationInsights-Go/blob/master/appinsights/contracts/contexttagkeys.go
#   # [outputs.application_insights.context_tag_sources]
#   #   "ai.cloud.role" = "kubernetes_container_name" "ai.cloud.roleInstance" = "kubernetes_pod_name"
# # Sends metrics to Azure Data Explorer
# [[outputs.azure_data_explorer]]
#   ## The URI property of the Azure Data Explorer resource on Azure ex: endpoint_url = https://myadxresource.australiasoutheast.kusto.windows.net
#   endpoint_url = ""
#
#   ## The Azure Data Explorer database that the metrics will be ingested into. The plugin will NOT generate this database automatically, it's expected that this database already exists before 
#   ## ingestion. ex: "exampledatabase"
#   database = ""
#
#   ## Timeout for Azure Data Explorer operations
#   # timeout = "20s"
#
#   ## Type of metrics grouping used when pushing to Azure Data Explorer. Default is "TablePerMetric" for one table per different metric. For more information, please check the plugin README.
#   # metrics_grouping_type = "TablePerMetric"
#
#   ## Name of the single table to store all the metrics (Only needed if metrics_grouping_type is "SingleTable").
#   # table_name = ""
#
#   ## Creates tables and relevant mapping if set to true(default). Skips table and mapping creation if set to false, this is useful for running Telegraf with the lowest possible permissions i.e. 
#   ## table ingestor role.
#   # create_tables = true
#
#   ##  Ingestion method to use. Available options are
#   ##    - managed -- streaming ingestion with fallback to batched ingestion or the "queued" method below - queued -- queue up metrics data and process sequentially
#   # ingestion_type = "queued"
# # Send aggregate metrics to Azure Monitor
# [[outputs.azure_monitor]]
#   ## Timeout for HTTP writes.
#   # timeout = "20s"
#
#   ## Set the namespace prefix, defaults to "Telegraf/<input-name>".
#   # namespace_prefix = "Telegraf/"
#
#   ## Azure Monitor doesn't have a string value type, so convert string fields to dimensions (a.k.a. tags) if enabled. Azure Monitor allows a maximum of 10 dimensions so Telegraf will only send the 
#   ## first 10 alphanumeric dimensions.
#   # strings_as_dimensions = false
#
#   ## Both region and resource_id must be set or be available via the Instance Metadata service on Azure Virtual Machines.
#   #
#   ## Azure Region to publish metrics against.
#   ##   ex: region = "southcentralus"
#   # region = ""
#   #
#   ## The Azure Resource ID against which metric will be logged, e.g.
#   ##   ex: resource_id = "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>"
#   # resource_id = ""
#
#   ## Optionally, if in Azure US Government, China, or other sovereign cloud environment, set the appropriate REST endpoint for receiving metrics. (Note: region may be unused in this context)
#   # endpoint_url = "https://monitoring.core.usgovcloudapi.net"
# # Configuration for Google Cloud BigQuery to send entries
# [[outputs.bigquery]]
#   ## Credentials File
#   credentials_file = "/path/to/service/account/key.json"
#
#   ## Google Cloud Platform Project
#   project = "my-gcp-project"
#
#   ## The namespace for the metric descriptor
#   dataset = "telegraf"
#
#   ## Timeout for BigQuery operations.
#   # timeout = "5s"
#
#   ## Character to replace hyphens on Metric name
#   # replace_hyphen_to = "_"
# # Publish Telegraf metrics to a Google Cloud PubSub topic
# [[outputs.cloud_pubsub]]
#   ## Required. Name of Google Cloud Platform (GCP) Project that owns the given PubSub topic.
#   project = "my-project"
#
#   ## Required. Name of PubSub topic to publish metrics to.
#   topic = "my-topic"
#
#   ## Required. Data format to consume. Each data format has its own unique set of configuration options. Read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"
#
#   ## Optional. Filepath for GCP credentials JSON file to authorize calls to PubSub APIs. If not set explicitly, Telegraf will attempt to use Application Default Credentials, which is preferred.
#   # credentials_file = "path/to/my/creds.json"
#
#   ## Optional. If true, will send all metrics per write in one PubSub message.
#   # send_batched = true
#
#   ## The following publish_* parameters specifically configures batching requests made to the GCP Cloud PubSub API via the PubSub Golang library. Read more here: 
#   ## https://godoc.org/cloud.google.com/go/pubsub#PublishSettings
#
#   ## Optional. Send a request to PubSub (i.e. actually publish a batch) when it has this many PubSub messages. If send_batched is true, this is ignored and treated as if it were 1.
#   # publish_count_threshold = 1000
#
#   ## Optional. Send a request to PubSub (i.e. actually publish a batch) when it has this many PubSub messages. If send_batched is true, this is ignored and treated as if it were 1
#   # publish_byte_threshold = 1000000
#
#   ## Optional. Specifically configures requests made to the PubSub API.
#   # publish_num_go_routines = 2
#
#   ## Optional. Specifies a timeout for requests to the PubSub API.
#   # publish_timeout = "30s"
#
#   ## Optional. If true, published PubSub message data will be base64-encoded.
#   # base64_data = false
#
#   ## Optional. PubSub attributes to add to metrics.
#   # [outputs.cloud_pubsub.attributes]
#   #   my_attr = "tag_value"
# # Configuration for AWS CloudWatch output.
# [[outputs.cloudwatch]]
#   ## Amazon REGION
#   region = "us-east-1"
#
#   ## Amazon Credentials Credentials are loaded in the following order 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified 2) Assumed credentials via 
#   ## STS if role_arn is specified 3) explicit credentials from 'access_key' and 'secret_key' 4) shared profile from 'profile' 5) environment variables 6) shared credentials file 7) EC2 Instance 
#   ## Profile
#   #access_key = "" secret_key = "" token = "" role_arn = "" web_identity_token_file = "" role_session_name = "" profile = "" shared_credential_file = ""
#
#   ## Endpoint to make request against, the correct endpoint is automatically determined and this option should only be set if you wish to override the default.
#   ##   ex: endpoint_url = "http://localhost:8000"
#   # endpoint_url = ""
#
#   ## Set http_proxy
#   # use_system_proxy = false http_proxy_url = "http://localhost:8888"
#
#   ## Namespace for the CloudWatch MetricDatums
#   namespace = "InfluxData/Telegraf"
#
#   ## If you have a large amount of metrics, you should consider to send statistic values instead of raw metrics which could not only improve performance but also save AWS API cost. If enable this 
#   ## flag, this plugin would parse the required CloudWatch statistic fields (count, min, max, and sum) and send them to CloudWatch. You could use basicstats aggregator to calculate those fields. 
#   ## If not all statistic fields are available, all fields would still be sent as raw metrics.
#   # write_statistics = false
#
#   ## Enable high resolution metrics of 1 second (if not enabled, standard resolution are of 60 seconds precision)
#   # high_resolution_metrics = false
# # Configuration for AWS CloudWatchLogs output.
# [[outputs.cloudwatch_logs]]
#   ## The region is the Amazon region that you wish to connect to. Examples include but are not limited to: - us-west-1 - us-west-2 - us-east-1 - ap-southeast-1 - ap-southeast-2 ...
#   region = "us-east-1"
#
#   ## Amazon Credentials Credentials are loaded in the following order 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified 2) Assumed credentials via 
#   ## STS if role_arn is specified 3) explicit credentials from 'access_key' and 'secret_key' 4) shared profile from 'profile' 5) environment variables 6) shared credentials file 7) EC2 Instance 
#   ## Profile
#   #access_key = "" secret_key = "" token = "" role_arn = "" web_identity_token_file = "" role_session_name = "" profile = "" shared_credential_file = ""
#
#   ## Endpoint to make request against, the correct endpoint is automatically determined and this option should only be set if you wish to override the default.
#   ##   ex: endpoint_url = "http://localhost:8000"
#   # endpoint_url = ""
#
#   ## Cloud watch log group. Must be created in AWS cloudwatch logs upfront! For example, you can specify the name of the k8s cluster here to group logs from all cluster in oine place
#   log_group = "my-group-name"
#
#   ## Log stream in log group Either log group name or reference to metric attribute, from which it can be parsed: tag:<TAG_NAME> or field:<FIELD_NAME>. If log stream is not exist, it will be 
#   ## created. Since AWS is not automatically delete logs streams with expired logs entries (i.e. empty log stream) you need to put in place appropriate house-keeping 
#   ## (https://forums.aws.amazon.com/thread.jspa?threadID=178855)
#   log_stream = "tag:location"
#
#   ## Source of log data - metric name specify the name of the metric, from which the log data should be retrieved. I.e., if you are using docker_log plugin to stream logs from container, then 
#   ## specify log_data_metric_name = "docker_log"
#   log_data_metric_name = "docker_log"
#
#   ## Specify from which metric attribute the log data should be retrieved: tag:<TAG_NAME> or field:<FIELD_NAME>. I.e., if you are using docker_log plugin to stream logs from container, then 
#   ## specify log_data_source = "field:message"
#   log_data_source = "field:message"
# # Configuration for CrateDB to send metrics to.
# [[outputs.cratedb]]
#   # A github.com/jackc/pgx/v4 connection string. See https://pkg.go.dev/github.com/jackc/pgx/v4#ParseConfig
#   url = "postgres://user:password@localhost/schema?sslmode=disable"
#   # Timeout for all CrateDB queries.
#   timeout = "5s"
#   # Name of the table to store metrics in.
#   table = "metrics"
#   # If true, and the metrics table does not exist, create it automatically.
#   table_create = true
#   # The character(s) to replace any '.' in an object key with
#   key_separator = "_"
# # Configuration for DataDog API to send metrics to.
# [[outputs.datadog]]
#   ## Datadog API key
#   apikey = "my-secret-key"
#
#   ## Connection timeout.
#   # timeout = "5s"
#
#   ## Write URL override; useful for debugging.
#   # url = "https://app.datadoghq.com/api/v1/series"
#
#   ## Set http_proxy
#   # use_system_proxy = false http_proxy_url = "http://localhost:8888"
#
#   ## Override the default (none) compression used to send data. Supports: "zlib", "none"
#   # compression = "none"
# # Send metrics to nowhere at all
# [[outputs.discard]]
#   # no configuration
# # Send telegraf metrics to a Dynatrace environment
# [[outputs.dynatrace]]
#   ## For usage with the Dynatrace OneAgent you can omit any configuration, the only requirement is that the OneAgent is running on the same host. Only setup environment url and token if you want 
#   ## to monitor a Host without the OneAgent present.
#   ##
#   ## Your Dynatrace environment URL. For Dynatrace OneAgent you can leave this empty or set it to "http://127.0.0.1:14499/metrics/ingest" (default) For Dynatrace SaaS environments the URL scheme 
#   ## is "https://{your-environment-id}.live.dynatrace.com/api/v2/metrics/ingest" For Dynatrace Managed environments the URL scheme is 
#   ## "https://{your-domain}/e/{your-environment-id}/api/v2/metrics/ingest"
#   url = ""
#
#   ## Your Dynatrace API token. Create an API token within your Dynatrace environment, by navigating to Settings > Integration > Dynatrace API The API token needs data ingest scope permission. When 
#   ## using OneAgent, no API token is required.
#   api_token = ""
#
#   ## Optional prefix for metric names (e.g.: "telegraf")
#   prefix = "telegraf"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Optional flag for ignoring tls certificate check
#   # insecure_skip_verify = false
#
#   ## Connection timeout, defaults to "5s" if not set.
#   timeout = "5s"
#
#   ## If you want metrics to be treated and reported as delta counters, add the metric names here
#   additional_counters = [ ]
#
#   ## Optional dimensions to be added to every metric
#   # [outputs.dynatrace.default_dimensions] default_key = "default value"
# # Configuration for Elasticsearch to send metrics to.
# [[outputs.elasticsearch]]
#   ## The full HTTP endpoint URL for your Elasticsearch instance Multiple urls can be specified as part of the same cluster, this means that only ONE of the urls will be written to each interval
#   urls = [ "http://node1.es.example.com:9200" ] # required.
#   ## Elasticsearch client timeout, defaults to "5s" if not set.
#   timeout = "5s"
#   ## Set to true to ask Elasticsearch a list of all cluster nodes, thus it is not necessary to list all nodes in the urls config option
#   enable_sniffer = false
#   ## Set to true to enable gzip compression
#   enable_gzip = false
#   ## Set the interval to check if the Elasticsearch nodes are available Setting to "0s" will disable the health check (not recommended in production)
#   health_check_interval = "10s"
#   ## Set the timeout for periodic health checks.
#   # health_check_timeout = "1s"
#   ## HTTP basic authentication details. HTTP basic authentication details
#   # username = "telegraf" password = "mypassword"
#   ## HTTP bearer token authentication details
#   # auth_bearer_token = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9"
#
#   ## Index Config The target index for metrics (Elasticsearch will create if it not exists). You can use the date specifiers below to create indexes per time frame. The metric timestamp will be 
#   ## used to decide the destination index name
#   # %Y - year (2016) %y - last two digits of year (00..99) %m - month (01..12) %d - day of month (e.g., 01) %H - hour (00..23) %V - week of the year (ISO week) (01..53)
#   ## Additionally, you can specify a tag name using the notation {{tag_name}} which will be used as part of the index name. If the tag does not exist, the default tag value will be used.
#   # index_name = "telegraf-{{host}}-%Y.%m.%d" default_tag_value = "none"
#   index_name = "telegraf-%Y.%m.%d" # required.
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Template Config Set to true if you want telegraf to manage its index template. If enabled it will create a recommended index template for telegraf indexes
#   manage_template = true
#   ## The template name used for telegraf indexes
#   template_name = "telegraf"
#   ## Set to true if you want telegraf to overwrite an existing template
#   overwrite_template = false
#   ## If set to true a unique ID hash will be sent as sha256(concat(timestamp,measurement,series-hash)) string it will enable data resend and update metric points avoiding duplicated metrics with 
#   ## diferent id's
#   force_document_id = false
#
#   ## Specifies the handling of NaN and Inf values. This option can have the following values:
#   ##    none -- do not modify field-values (default); will produce an error if NaNs or infs are encountered drop -- drop fields containing NaNs or infs replace -- replace with the value in 
#   ##    "float_replacement_value" (default: 0.0)
#   ##               NaNs and inf will be replaced with the given number, -inf with the negative of that number
#   # float_handling = "none" float_replacement_value = 0.0
#
#   ## Pipeline Config To use a ingest pipeline, set this to the name of the pipeline you want to use.
#   # use_pipeline = "my_pipeline"
#   ## Additionally, you can specify a tag name using the notation {{tag_name}} which will be used as part of the pipeline name. If the tag does not exist, the default pipeline will be used as the 
#   ## pipeline. If no default pipeline is set, no pipeline is used for the metric.
#   # use_pipeline = "{{es_pipeline}}" default_pipeline = "my_pipeline"
# # Configuration for Event Hubs output plugin
# [[outputs.event_hubs]]
#   ## The full connection string to the Event Hub (required) The shared access key must have "Send" permissions on the target Event Hub.
#   connection_string = "Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName"
#
#   ## Client timeout (defaults to 30s)
#   # timeout = "30s"
#
#   ## Partition key Metric tag or field name to use for the event partition key. The value of this tag or field is set as the key for events if it exists. If both, tag and field, exist the tag is 
#   ## preferred.
#   # partition_key = ""
#
#   ## Set the maximum batch message size in bytes The allowable size depends on the Event Hub tier See: 
#   ## https://learn.microsoft.com/azure/event-hubs/event-hubs-quotas#basic-vs-standard-vs-premium-vs-dedicated-tiers Setting this to 0 means using the default size from the Azure Event Hubs Client 
#   ## library (1000000 bytes)
#   # max_message_size = 1000000
#
#   ## Data format to output. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "json"
# # Send metrics to command as input over stdin
# [[outputs.exec]]
#   ## Command to ingest metrics via stdin.
#   command = ["tee", "-a", "/dev/null"]
#
#   ## Environment variables Array of "key=value" pairs to pass as environment variables e.g. "KEY=value", "USERNAME=John Doe", "LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs"
#   # environment = []
#
#   ## Timeout for command to complete.
#   # timeout = "5s"
#
#   ## Data format to output. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   # data_format = "influx"
# # Run executable as long-running output plugin
# [[outputs.execd]]
#   ## One program to run as daemon. NOTE: process and each argument should each be their own string
#   command = ["my-telegraf-output", "--some-flag", "value"]
#
#   ## Environment variables Array of "key=value" pairs to pass as environment variables e.g. "KEY=value", "USERNAME=John Doe", "LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs"
#   # environment = []
#
#   ## Delay before the process is restarted after an unexpected termination
#   restart_delay = "10s"
#
#   ## Flag to determine whether execd should throw error when part of metrics is unserializable Setting this to true will skip the unserializable metrics and process the rest of metrics Setting 
#   ## this to false will throw error when encountering unserializable metrics and none will be processed
#   # ignore_serialization_error = false
#
#   ## Data format to export. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "influx"
# # Send telegraf metrics to file(s)
# [[outputs.file]]
#   ## Files to write to, "stdout" is a specially handled file.
#   files = ["stdout", "/tmp/metrics.out"]
#
#   ## Use batch serialization format instead of line based delimiting.  The batch format allows for the production of non line based output formats and may more efficiently encode and write 
#   ## metrics.
#   # use_batch_format = false
#
#   ## The file will be rotated after the time interval specified.  When set to 0 no time based rotation is performed.
#   # rotation_interval = "0h"
#
#   ## The logfile will be rotated when it becomes larger than the specified size.  When set to 0 no size based rotation is performed.
#   # rotation_max_size = "0MB"
#
#   ## Maximum number of rotated archives to keep, any older logs are deleted. If set to -1, no archives are removed.
#   # rotation_max_archives = 5
#
#   ## Data format to output. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "influx"
# # Configuration for Graphite server to send metrics to
# [[outputs.graphite]]
#   ## TCP endpoint for your graphite instance. If multiple endpoints are configured, the output will be load balanced. Only one of the endpoints will be written to with each iteration.
#   servers = ["localhost:2003"]
#   ## Prefix metrics name
#   prefix = ""
#   ## Graphite output template see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   template = "host.tags.measurement.field"
#
#   ## Strict sanitization regex This is the default sanitization regex that is used on data passed to the graphite serializer. Users can add additional characters here if required. Be aware that 
#   ## the characters, '/' '@' '*' are always replaced with '_', '..' is replaced with '.', and '\' is removed even if added to the following regex.
#   # graphite_strict_sanitize_regex = '[^a-zA-Z0-9-:._=\p{L}]'
#
#   ## Enable Graphite tags support
#   # graphite_tag_support = false
#
#   ## Applied sanitization mode when graphite tag support is enabled. * strict - uses the regex specified above * compatible - allows for greater number of characters
#   # graphite_tag_sanitize_mode = "strict"
#
#   ## Character for separating metric name and field for Graphite tags
#   # graphite_separator = "."
#
#   ## Graphite templates patterns 1. Template for cpu 2. Template for disk* 3. Default template
#   # templates = [
#   #  "cpu tags.measurement.host.field", "disk* measurement.field", "host.measurement.tags.field" ]
#
#   ## timeout in seconds for the write connection to graphite
#   timeout = 2
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Send telegraf metrics to graylog
# [[outputs.graylog]]
#   ## Endpoints for your graylog instances.
#   servers = ["udp://127.0.0.1:12201"]
#
#   ## Connection timeout.
#   # timeout = "5s"
#
#   ## The field to use as the GELF short_message, if unset the static string "telegraf" will be used.
#   ##   example: short_message_field = "message"
#   # short_message_field = ""
#
#   ## According to GELF payload specification, additional fields names must be prefixed with an underscore. Previous versions did not prefix custom field 'name' with underscore. Set to true for 
#   ## backward compatibility.
#   # name_field_no_prefix = false
#
#   ## Connection retry options Attempt to connect to the enpoints if the initial connection fails. If 'false', Telegraf will give up after 3 connection attempt and will exit with an error. If set 
#   ## to 'true', the plugin will retry to connect to the unconnected endpoints infinitely.
#   # connection_retry = false
#   ## Time to wait between connection retry attempts.
#   # connection_retry_wait_time = "15s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Send telegraf metrics to GroundWork Monitor
# [[outputs.groundwork]]
#   ## URL of your groundwork instance.
#   url = "https://groundwork.example.com"
#
#   ## Agent uuid for GroundWork API Server.
#   agent_id = ""
#
#   ## Username and password to access GroundWork API.
#   username = "" password = ""
#
#   ## Default application type to use in GroundWork client
#   # default_app_type = "TELEGRAF"
#
#   ## Default display name for the host with services(metrics).
#   # default_host = "telegraf"
#
#   ## Default service state.
#   # default_service_state = "SERVICE_OK"
#
#   ## The name of the tag that contains the hostname.
#   # resource_tag = "host"
#
#   ## The name of the tag that contains the host group name.
#   # group_tag = "group"
# # Configurable HTTP health check resource based on metrics
# [[outputs.health]]
#   ## Address and port to listen on.
#   ##   ex: service_address = "http://localhost:8080"
#   ##       service_address = "unix:///var/run/telegraf-health.sock"
#   # service_address = "http://:8080"
#
#   ## The maximum duration for reading the entire request.
#   # read_timeout = "5s"
#   ## The maximum duration for writing the entire response.
#   # write_timeout = "5s"
#
#   ## Username and password to accept for HTTP basic authentication.
#   # basic_username = "user1" basic_password = "secret"
#
#   ## Allowed CA certificates for client certificates.
#   # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## TLS server certificate and private key.
#   # tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#
#   ## One or more check sub-tables should be defined, it is also recommended to use metric filtering to limit the metrics that flow into this output.
#   ##
#   ## When using the default buffer sizes, this example will fail when the metric buffer is half full.
#   ##
#   ## namepass = ["internal_write"] tagpass = { output = ["influxdb"] }
#   ##
#   ## [[outputs.health.compares]]
#   ##   field = "buffer_size" lt = 5000.0
#   ##
#   ## [[outputs.health.contains]]
#   ##   field = "buffer_size"
# # A plugin that can transmit metrics over HTTP
# [[outputs.http]]
#   ## URL is the address to send metrics to
#   url = "http://127.0.0.1:8080/telegraf"
#
#   ## Timeout for HTTP message
#   # timeout = "5s"
#
#   ## HTTP method, one of: "POST" or "PUT"
#   # method = "POST"
#
#   ## HTTP Basic Auth credentials
#   # username = "username" password = "pa$$word"
#
#   ## OAuth2 Client Credentials Grant
#   # client_id = "clientid" client_secret = "secret" token_url = "https://indentityprovider/oauth2/v1/token" audience = "" scopes = ["urn:opc:idm:__myscopes__"]
#
#   ## Goole API Auth
#   # google_application_credentials = "/etc/telegraf/example_secret.json"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional Cookie authentication
#   # cookie_auth_url = "https://localhost/authMe" cookie_auth_method = "POST" cookie_auth_username = "username" cookie_auth_password = "pa$$word" cookie_auth_headers = '{"Content-Type": 
#   # "application/json", "X-MY-HEADER":"hello"}' cookie_auth_body = '{"username": "user", "password": "pa$$word", "authenticate": "me"}'
#   ## cookie_auth_renewal not set or set to "0" will auth once and never renew the cookie
#   # cookie_auth_renewal = "5m"
#
#   ## Data format to output. Each data format has it's own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   # data_format = "influx"
#
#   ## Use batch serialization format (default) instead of line based format. Batch format is more efficient and should be used unless line based format is really needed.
#   # use_batch_format = true
#
#   ## HTTP Content-Encoding for write request body, can be set to "gzip" to compress body or "identity" to apply no encoding.
#   # content_encoding = "identity"
#
#   ## Additional HTTP headers
#   # [outputs.http.headers]
#   #   # Should be set manually to "application/json" for json data_format
#   #   Content-Type = "text/plain; charset=utf-8"
#
#   ## MaxIdleConns controls the maximum number of idle (keep-alive) connections across all hosts. Zero means no limit.
#   # max_idle_conn = 0
#
#   ## MaxIdleConnsPerHost, if non-zero, controls the maximum idle (keep-alive) connections to keep per-host. If zero, DefaultMaxIdleConnsPerHost is used(2).
#   # max_idle_conn_per_host = 2
#
#   ## Idle (keep-alive) connection timeout. Maximum amount of time before idle connection is closed. Zero means no limit.
#   # idle_conn_timeout = 0
#
#   ## Amazon Region
#   #region = "us-east-1"
#
#   ## Amazon Credentials Amazon Credentials are not attempted unless aws_service is set to a non-empty string and may need to match the name of the service you're ouputting to
#   #aws_service = "execute-api"
#
#   ## Credentials are loaded in the following order 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified 2) Assumed credentials via STS if role_arn is 
#   ## specified 3) explicit credentials from 'access_key' and 'secret_key' 4) shared profile from 'profile' 5) environment variables 6) shared credentials file 7) EC2 Instance Profile
#   #access_key = "" secret_key = "" token = "" role_arn = "" web_identity_token_file = "" role_session_name = "" profile = "" shared_credential_file = ""
#
#   ## Optional list of statuscodes (<200 or >300) upon which requests should not be retried
#   # non_retryable_statuscodes = [409, 413]
# # Configuration for sending metrics to InfluxDB
 [[outputs.influxdb]]
#   ## The full HTTP or UDP URL for your InfluxDB instance.
#   ##
#   ## Multiple URLs can be specified for a single cluster, only ONE of the urls will be written to each interval.
# urls = ["unix:///var/run/influxdb.sock"]
#   # urls = ["udp://127.0.0.1:8089"]
 urls = ["http://192.168.14.200:8086"]
# urls = ["http://82.142.167.46:49001"]
#
#   ## The target database for metrics; will be created as needed. For UDP url endpoint database needs to be configured on server side.
 database = "monitoring_telegraf"
#
#   ## The value of this tag will be used to determine the database.  If this tag is not set the 'database' option is used as the default.
#   # database_tag = ""
#
#   ## If true, the 'database_tag' will not be included in the written metric.
#   # exclude_database_tag = false
#
#   ## If true, no CREATE DATABASE queries will be sent.  Set to true when using Telegraf with a user without permissions to create databases or when the database already exists.
#   # skip_database_creation = false
#
#   ## Name of existing retention policy to write to.  Empty string writes to the default retention policy.  Only takes effect when using HTTP.
#   # retention_policy = ""
#
#   ## The value of this tag will be used to determine the retention policy.  If this tag is not set the 'retention_policy' option is used as the default.
#   # retention_policy_tag = ""
#
#   ## If true, the 'retention_policy_tag' will not be included in the written metric.
#   # exclude_retention_policy_tag = false
#
#   ## Write consistency (clusters only), can be: "any", "one", "quorum", "all". Only takes effect when using HTTP.
#   # write_consistency = "any"
#
#   ## Timeout for HTTP messages.
    timeout = "30s"
#
#   ## HTTP Basic Auth
#   # username = "telegraf" password = "metricsmetricsmetricsmetrics"
#
#   ## HTTP User-Agent
#   # user_agent = "telegraf"
#
#   ## UDP payload size is the maximum packet size to send.
#   # udp_payload = "512B"
#
#   ## Optional TLS Config for use on HTTP connections.
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## HTTP Proxy override, if unset values the standard proxy environment variables are consulted to determine which proxy, if any, should be used.
#   # http_proxy = "http://corporate.proxy:3128"
#
#   ## Additional HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## HTTP Content-Encoding for write request body, can be set to "gzip" to compress body or "identity" to apply no encoding.
#   # content_encoding = "gzip"
#
#   ## When true, Telegraf will output unsigned integers as unsigned values, i.e.: "42u".  You will need a version of InfluxDB supporting unsigned integer values.  Enabling this option will result 
#   ## in field type errors if existing data has been written.
#   # influx_uint_support = false
# # Configuration for sending metrics to an Instrumental project
# [[outputs.instrumental]]
#   ## Project API Token (required)
#   api_token = "API Token" # required
#   ## Prefix the metrics with a given name
#   prefix = ""
#   ## Stats output template (Graphite formatting) see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite
#   template = "host.tags.measurement.field"
#   ## Timeout in seconds to connect
#   timeout = "2s"
#   ## Debug true - Print communication to Instrumental
#   debug = false
# # Save metrics to an IoTDB Database
# [[outputs.iotdb]]
#   ## Configuration of IoTDB server connection
#   host = "127.0.0.1"
#   # port = "6667"
#
#   ## Configuration of authentication
#   # user = "root" password = "root"
#
#   ## Timeout to open a new session. A value of zero means no timeout.
#   # timeout = "5s"
#
#   ## Configuration of type conversion for 64-bit unsigned int IoTDB currently DOES NOT support unsigned integers (version 13.x). 32-bit unsigned integers are safely converted into 64-bit signed 
#   ## integers by the plugin, however, this is not true for 64-bit values in general as overflows may occur. The following setting allows to specify the handling of 64-bit unsigned integers. 
#   ## Available values are:
#   ##   - "int64" -- convert to 64-bit signed integers and accept overflows - "int64_clip" -- convert to 64-bit signed integers and clip the values on overflow to 9,223,372,036,854,775,807 - "text" 
#   ##   -- convert to the string representation of the value
#   # uint64_conversion = "int64_clip"
#
#   ## Configuration of TimeStamp TimeStamp is always saved in 64bits int. timestamp_precision specifies the unit of timestamp. Available value: "second", "millisecond", "microsecond", 
#   ## "nanosecond"(default)
#   # timestamp_precision = "nanosecond"
#
#   ## Handling of tags Tags are not fully supported by IoTDB. A guide with suggestions on how to handle tags can be found here:
#   ##     https://iotdb.apache.org/UserGuide/Master/API/InfluxDB-Protocol.html
#   ##
#   ## Available values are:
#   ##   - "fields" -- convert tags to fields in the measurement - "device_id" -- attach tags to the device ID
#   ##
#   ## For Example, a metric named "root.sg.device" with the tags `tag1: "private"` and `tag2: "working"` and
#   ##  fields `s1: 100` and `s2: "hello"` will result in the following representations in IoTDB
#   ##   - "fields" -- root.sg.device, s1=100, s2="hello", tag1="private", tag2="working" - "device_id" -- root.sg.device.private.working, s1=100, s2="hello"
#   # convert_tags_to = "device_id"
# # Configuration for the Kafka server to send metrics to
# [[outputs.kafka]]
#   ## URLs of kafka brokers The brokers listed here are used to connect to collect metadata about a cluster. However, once the initial metadata collect is completed, telegraf will communicate 
#   ## solely with the kafka leader and not all defined brokers.
#   brokers = ["localhost:9092"]
#
#   ## Kafka topic for producer messages
#   topic = "telegraf"
#
#   ## The value of this tag will be used as the topic.  If not set the 'topic' option is used.
#   # topic_tag = ""
#
#   ## If true, the 'topic_tag' will be removed from to the metric.
#   # exclude_topic_tag = false
#
#   ## Optional Client id
#   # client_id = "Telegraf"
#
#   ## Set the minimal supported Kafka version.  Setting this enables the use of new Kafka features and APIs.  Of particular interested, lz4 compression requires at least version 0.10.0.0.
#   ##   ex: version = "1.1.0"
#   # version = ""
#
#   ## Optional topic suffix configuration. If the section is omitted, no suffix is used. Following topic suffix methods are supported:
#   ##   measurement - suffix equals to separator + measurement's name tags - suffix equals to separator + specified tags' values
#   ##                 interleaved with separator
#
#   ## Suffix equals to "_" + measurement name
#   # [outputs.kafka.topic_suffix]
#   #   method = "measurement" separator = "_"
#
#   ## Suffix equals to "__" + measurement's "foo" tag value.
#   ##   If there's no such a tag, suffix equals to an empty string
#   # [outputs.kafka.topic_suffix]
#   #   method = "tags" keys = ["foo"] separator = "__"
#
#   ## Suffix equals to "_" + measurement's "foo" and "bar"
#   ##   tag values, separated by "_". If there is no such tags, their values treated as empty strings.
#   # [outputs.kafka.topic_suffix]
#   #   method = "tags" keys = ["foo", "bar"] separator = "_"
#
#   ## The routing tag specifies a tagkey on the metric whose value is used as the message key.  The message key is used to determine which partition to send the message to.  This tag is prefered 
#   ## over the routing_key option.
#   routing_tag = "host"
#
#   ## The routing key is set as the message key and used to determine which partition to send the message to.  This value is only used when no routing_tag is set or as a fallback when the tag 
#   ## specified in routing tag is not found.
#   ##
#   ## If set to "random", a random value will be generated for each message.
#   ##
#   ## When unset, no message key is added and each message is routed to a random partition.
#   ##
#   ##   ex: routing_key = "random"
#   ##       routing_key = "telegraf"
#   # routing_key = ""
#
#   ## Compression codec represents the various compression codecs recognized by Kafka in messages.
#   ##  0 : None 1 : Gzip 2 : Snappy 3 : LZ4 4 : ZSTD
#   # compression_codec = 0
#
#   ## Idempotent Writes If enabled, exactly one copy of each message is written.
#   # idempotent_writes = false
#
#   ##  RequiredAcks is used in Produce Requests to tell the broker how many replica acknowledgements it must see before responding
#   ##   0 : the producer never waits for an acknowledgement from the broker.
#   ##       This option provides the lowest latency but the weakest durability guarantees (some data will be lost when a server fails). 1 : the producer gets an acknowledgement after the leader 
#   ##   replica has
#   ##       received the data. This option provides better durability as the client waits until the server acknowledges the request as successful (only messages that were written to the now-dead 
#   ##   leader but not yet replicated will be lost). -1: the producer gets an acknowledgement after all in-sync replicas have
#   ##       received the data. This option provides the best durability, we guarantee that no messages will be lost as long as at least one in sync replica remains.
#   # required_acks = -1
#
#   ## The maximum number of times to retry sending a metric before failing until the next flush.
#   # max_retry = 3
#
#   ## The maximum permitted size of a message. Should be set equal to or smaller than the broker's 'message.max.bytes'.
#   # max_message_bytes = 1000000
#
#   ## Optional TLS Config
#   # enable_tls = false tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Period between keep alive probes. Defaults to the OS configuration if not specified or zero.
#   # keep_alive_period = "15s"
#
#   ## Optional SOCKS5 proxy to use when connecting to brokers
#   # socks5_enabled = true socks5_address = "127.0.0.1:1080" socks5_username = "alice" socks5_password = "pass123"
#
#   ## Optional SASL Config
#   # sasl_username = "kafka" sasl_password = "secret"
#
#   ## Optional SASL: one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI (defaults to PLAIN)
#   # sasl_mechanism = ""
#
#   ## used if sasl_mechanism is GSSAPI (experimental)
#   # sasl_gssapi_service_name = ""
#   # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH
#   # sasl_gssapi_auth_type = "KRB5_USER_AUTH" sasl_gssapi_kerberos_config_path = "/" sasl_gssapi_realm = "realm" sasl_gssapi_key_tab_path = "" sasl_gssapi_disable_pafxfast = false
#
#   ## used if sasl_mechanism is OAUTHBEARER (experimental)
#   # sasl_access_token = ""
#
#   ## SASL protocol version.  When connecting to Azure EventHub set to 0.
#   # sasl_version = 1
#
#   # Disable Kafka metadata full fetch metadata_full = false
#
#   ## Data format to output. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   # data_format = "influx"
# # Configuration for the AWS Kinesis output.
# [[outputs.kinesis]]
#   ## Amazon REGION of kinesis endpoint.
#   region = "ap-southeast-2"
#
#   ## Amazon Credentials Credentials are loaded in the following order 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified 2) Assumed credentials via 
#   ## STS if role_arn is specified 3) explicit credentials from 'access_key' and 'secret_key' 4) shared profile from 'profile' 5) environment variables 6) shared credentials file 7) EC2 Instance 
#   ## Profile
#   #access_key = "" secret_key = "" token = "" role_arn = "" web_identity_token_file = "" role_session_name = "" profile = "" shared_credential_file = ""
#
#   ## Endpoint to make request against, the correct endpoint is automatically determined and this option should only be set if you wish to override the default.
#   ##   ex: endpoint_url = "http://localhost:8000"
#   # endpoint_url = ""
#
#   ## Kinesis StreamName must exist prior to starting telegraf.
#   streamname = "StreamName"
#
#   ## The partition key can be calculated using one of several methods:
#   ##
#   ## Use a static value for all writes:
#   #  [outputs.kinesis.partition]
#   #    method = "static" key = "howdy"
#   #
#   ## Use a random partition key on each write:
#   #  [outputs.kinesis.partition]
#   #    method = "random"
#   #
#   ## Use the measurement name as the partition key:
#   #  [outputs.kinesis.partition]
#   #    method = "measurement"
#   #
#   ## Use the value of a tag for all writes, if the tag is not set the empty default option will be used. When no default, defaults to "telegraf"
#   #  [outputs.kinesis.partition]
#   #    method = "tag" key = "host" default = "mykey"
#
#
#   ## Data format to output. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "influx"
#
#   ## debug will show upstream aws messages.
#   debug = false
# # Configuration for Librato API to send metrics to.
# [[outputs.librato]]
#   ## Librato API Docs http://dev.librato.com/v1/metrics-authentication Librato API user
#   api_user = "telegraf@influxdb.com" # required.
#   ## Librato API token
#   api_token = "my-secret-token" # required.
#   ## Debug
#   # debug = false
#   ## Connection timeout.
#   # timeout = "5s"
#   ## Output source Template (same as graphite buckets) see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite This template is used in librato's source (not 
#   ## metric's name)
#   template = "host"
# # A plugin that can send metrics over HTTPs to Logz.io
# [[outputs.logzio]]
#   ## Connection timeout, defaults to "5s" if not set.
#   # timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#
#   ## Logz.io account token
#   token = "your logz.io token" # required
#
#   ## Use your listener URL for your Logz.io account region.
#   # url = "https://listener.logz.io:8071"
# # A plugin that can transmit logs to Loki
# [[outputs.loki]]
#   ## The domain of Loki
#   domain = "https://loki.domain.tld"
#
#   ## Endpoint to write api
#   # endpoint = "/loki/api/v1/push"
#
#   ## Connection timeout, defaults to "5s" if not set.
#   # timeout = "5s"
#
#   ## Basic auth credential
#   # username = "loki" password = "pass"
#
#   ## Additional HTTP headers
#   # http_headers = {"X-Scope-OrgID" = "1"}
#
#   ## If the request must be gzip encoded
#   # gzip_request = false
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
# # A plugin that can transmit logs to mongodb
# [[outputs.mongodb]]
#   # connection string examples for mongodb
#   dsn = "mongodb://localhost:27017"
#   # dsn = "mongodb://mongod1:27017,mongod2:27017,mongod3:27017/admin&replicaSet=myReplSet&w=1"
#
#   # overrides serverSelectionTimeoutMS in dsn if set timeout = "30s"
#
#   # default authentication, optional authentication = "NONE"
#
#   # for SCRAM-SHA-256 authentication authentication = "SCRAM" username = "root" password = "***"
#
#   # for x509 certificate authentication authentication = "X509" tls_ca = "ca.pem" tls_key = "client.pem"
#   # # tls_key_pwd = "changeme" # required for encrypted tls_key
#   # insecure_skip_verify = false
#
#   # database to store measurements and time series collections database = "telegraf"
#
#   # granularity can be seconds, minutes, or hours. configuring this value will be based on your input collection frequency. see 
#   # https://docs.mongodb.com/manual/core/timeseries-collections/#create-a-time-series-collection granularity = "seconds"
#
#   # optionally set a TTL to automatically expire documents from the measurement collections. ttl = "360h"
# # Configuration for MQTT server to send metrics to
# [[outputs.mqtt]]
#   ## MQTT Brokers The list of brokers should only include the hostname or IP address and the port to the broker. This should follow the format `[{scheme}://]{host}:{port}`. For example, 
#   ## `localhost:1883` or `mqtt://localhost:1883`. Scheme can be any of the following: tcp://, mqtt://, tls://, mqtts:// non-TLS and TLS servers can not be mix-and-matched.
#   servers = ["localhost:1883", ] # or ["mqtts://tls.example.com:1883"]
#
#   ## Protocol can be `3.1.1` or `5`. Default is `3.1.1`
#   # procotol = "3.1.1"
#
#   ## MQTT Topic for Producer Messages MQTT outputs send metrics to this topic format: {{ .TopicPrefix }}/{{ .Hostname }}/{{ .PluginName }}/{{ .Tag "tag_key" }} (e.g. 
#   ## prefix/web01.example.com/mem/some_tag_value) Each path segment accepts either a template placeholder, an environment variable, or a tag key of the form `{{.Tag "tag_key_name"}}`. Empty path 
#   ## elements as well as special MQTT characters (such as `+` or `#`) are invalid to form the topic name and will lead to an error. In case a tag is missing in the metric, that path segment 
#   ## omitted for the final topic.
#   topic = "telegraf/{{ .Hostname }}/{{ .PluginName }}"
#
#   ## QoS policy for messages The mqtt QoS policy for sending messages. See https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.0.0/com.ibm.mq.dev.doc/q029090_.htm
#   ##   0 = at most once 1 = at least once 2 = exactly once
#   # qos = 2
#
#   ## Keep Alive Defines the maximum length of time that the broker and client may not communicate. Defaults to 0 which turns the feature off.
#   ##
#   ## For version v2.0.12 and later mosquitto there is a bug (see https://github.com/eclipse/mosquitto/issues/2117), which requires this to be non-zero. As a reference eclipse/paho.mqtt.golang 
#   ## defaults to 30.
#   # keep_alive = 0
#
#   ## username and password to connect MQTT server.
#   # username = "telegraf" password = "metricsmetricsmetricsmetrics"
#
#   ## client ID The unique client id to connect MQTT server. If this parameter is not set then a random ID is generated.
#   # client_id = ""
#
#   ## Timeout for write operations. default: 5s
#   # timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## When true, metrics will be sent in one MQTT message per flush. Otherwise, metrics are written one metric per MQTT message. DEPRECATED: Use layout option instead
#   # batch = false
#
#   ## When true, metric will have RETAIN flag set, making broker cache entries until someone actually reads it
#   # retain = false
#
#   ## Layout of the topics published. The following choices are available:
#   ##   non-batch -- send individual messages, one for each metric batch -- send all metric as a single message per MQTT topic NOTE: The following options will ignore the 'data_format' option and 
#   ## send single values
#   ##   field -- send individual messages for each field, appending its name to the metric topic homie-v4 -- send metrics with fields and tags according to the 4.0.0 specs
#   ##                see https://homieiot.github.io/specification/
#   # layout = "non-batch"
#
#   ## HOMIE specific settings The following options provide templates for setting the device name and the node-ID for the topics. Both options are MANDATORY and can contain {{ .PluginName }} 
#   ## (metric name), {{ .Tag "key"}} (tag reference to 'key') or constant strings. The templays MAY NOT contain slashes!
#   # homie_device_name = "" homie_node_id = ""
#
#   ## Each data format has its own unique set of configuration options, read more about them here: https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "influx"
#
#   ## Optional MQTT 5 publish properties These setting only apply if the "protocol" property is set to 5. This must be defined at the end of the plugin settings, otherwise TOML will assume anything 
#   ## else is part of this table. For more details on publish properties see the spec: https://docs.oasis-open.org/mqtt/mqtt/v5.0/os/mqtt-v5.0-os.html#_Toc3901109
#   # [outputs.mqtt.v5]
#   #   content_type = "" response_topic = "" message_expiry = "0s" topic_alias = 0 [outputs.mqtt.v5.user_properties] "key1" = "value 1" "key2" = "value 2"
# # Send telegraf measurements to NATS
# [[outputs.nats]]
#   ## URLs of NATS servers
#   servers = ["nats://localhost:4222"]
#
#   ## Optional client name
#   # name = ""
#
#   ## Optional credentials
#   # username = "" password = ""
#
#   ## Optional NATS 2.0 and NATS NGS compatible user credentials
#   # credentials = "/etc/telegraf/nats.creds"
#
#   ## NATS subject for producer messages
#   subject = "telegraf"
#
#   ## Use Transport Layer Security
#   # secure = false
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Data format to output. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "influx"
# # Send metrics to New Relic metrics endpoint
# [[outputs.newrelic]]
#   ## The 'insights_key' parameter requires a NR license key. New Relic recommends you create one with a convenient name such as TELEGRAF_INSERT_KEY. reference: 
#   ## https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#ingest-license-key
#   # insights_key = "New Relic License Key Here"
#
#   ## Prefix to add to add to metric name for easy identification. This is very useful if your metric names are ambiguous.
#   # metric_prefix = ""
#
#   ## Timeout for writes to the New Relic API.
#   # timeout = "15s"
#
#   ## HTTP Proxy override. If unset use values from the standard proxy environment variables to determine proxy, if any.
#   # http_proxy = "http://corporate.proxy:3128"
#
#   ## Metric URL override to enable geographic location endpoints.
#   # If not set use values from the standard metric_url = "https://metric-api.newrelic.com/metric/v1"
# # Send telegraf measurements to NSQD
# [[outputs.nsq]]
#   ## Location of nsqd instance listening on TCP
#   server = "localhost:4150"
#   ## NSQ topic for producer messages
#   topic = "telegraf"
#
#   ## Data format to output. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "influx"
# # Send OpenTelemetry metrics over gRPC
# [[outputs.opentelemetry]]
#   ## Override the default (localhost:4317) OpenTelemetry gRPC service address:port
#   # service_address = "localhost:4317"
#
#   ## Override the default (5s) request timeout
#   # timeout = "5s"
#
#   ## Optional TLS Config.
#   ##
#   ## Root certificates for verifying server certificates encoded in PEM format.
#   # tls_ca = "/etc/telegraf/ca.pem"
#   ## The public and private keypairs for the client encoded in PEM format. May contain intermediate certificates.
#   # tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS, but skip TLS chain and host verification.
#   # insecure_skip_verify = false
#   ## Send the specified TLS server name via SNI.
#   # tls_server_name = "foo.example.com"
#
#   ## Override the default (gzip) compression used to send data. Supports: "gzip", "none"
#   # compression = "gzip"
#
#   ## Configuration options for the Coralogix dialect Enable the following section of you use this plugin with a Coralogix endpoint
#   # [outputs.opentelemetry.coralogix]
#   #   ## Your Coralogix private key (required). Please note that this is sensitive data!
#   #   private_key = "your_coralogix_key"
#   #
#   #   ## Application and subsystem names for the metrics (required)
#   #   application = "$NAMESPACE" subsystem = "$HOSTNAME"
#
#   ## Additional OpenTelemetry resource attributes
#   # [outputs.opentelemetry.attributes] "service.name" = "demo"
#
#   ## Additional gRPC request metadata
#   # [outputs.opentelemetry.headers] key1 = "value1"
# # Configuration for OpenTSDB server to send metrics to
# [[outputs.opentsdb]]
#   ## prefix for metrics keys
#   prefix = "my.specific.prefix."
#
#   ## DNS name of the OpenTSDB server Using "opentsdb.example.com" or "tcp://opentsdb.example.com" will use the telnet API. "http://opentsdb.example.com" will use the Http API.
#   host = "opentsdb.example.com"
#
#   ## Port of the OpenTSDB server
#   port = 4242
#
#   ## Number of data points to send to OpenTSDB in Http requests. Not used with telnet API.
#   http_batch_size = 50
#
#   ## URI Path for Http requests to OpenTSDB. Used in cases where OpenTSDB is located behind a reverse proxy.
#   http_path = "/api/put"
#
#   ## Debug true - Prints OpenTSDB communication
#   debug = false
#
#   ## Separator separates measurement name from field
#   separator = "_"
# # Publishes metrics to a postgresql database
# ///////////////////// [[outputs.postgresql]]
#   ## Specify connection address via the standard libpq connection string:
#   ##   host=... user=... password=... sslmode=... dbname=... Or a URL: postgres://[user[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full] See 
#   ## https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING
#   ##
#   ## All connection parameters are optional. Environment vars are also supported. e.g. PGPASSWORD, PGHOST, PGUSER, PGDATABASE All supported vars can be found here:
#   ##  https://www.postgresql.org/docs/current/libpq-envars.html
#   ##
#   ## Non-standard parameters:
#   ##   pool_max_conns (default: 1) - Maximum size of connection pool for parallel (per-batch per-table) inserts. pool_min_conns (default: 0) - Minimum size of connection pool. 
#   ##   pool_max_conn_lifetime (default: 0s) - Maximum age of a connection before closing. pool_max_conn_idle_time (default: 0s) - Maximum idle time of a connection before closing. 
#   ##   pool_health_check_period (default: 0s) - Duration between health checks on idle connections.
#   # connection = ""
#
#   ## Postgres schema to use.
#   # schema = "public"
#
#   ## Store tags as foreign keys in the metrics table. Default is false.
#   # tags_as_foreign_keys = false
#
#   ## Suffix to append to table name (measurement name) for the foreign tag table.
#   # tag_table_suffix = "_tag"
#
#   ## Deny inserting metrics if the foreign tag can't be inserted.
#   # foreign_tag_constraint = false
#
#   ## Store all tags as a JSONB object in a single 'tags' column.
#   # tags_as_jsonb = false
#
#   ## Store all fields as a JSONB object in a single 'fields' column.
#   # fields_as_jsonb = false
#
#   ## Templated statements to execute when creating a new table.
#   # create_templates = [
#   #   '''CREATE TABLE {{ .table }} ({{ .columns }})''', ]
#
#   ## Templated statements to execute when adding columns to a table. Set to an empty list to disable. Points containing tags for which there is no column will be skipped. Points containing fields 
#   ## for which there is no column will have the field omitted.
#   # add_column_templates = [
#   #   '''ALTER TABLE {{ .table }} ADD COLUMN IF NOT EXISTS {{ .columns|join ", ADD COLUMN IF NOT EXISTS " }}''', ]
#
#   ## Templated statements to execute when creating a new tag table.
#   # tag_table_create_templates = [
#   #   '''CREATE TABLE {{ .table }} ({{ .columns }}, PRIMARY KEY (tag_id))''', ]
#
#   ## Templated statements to execute when adding columns to a tag table. Set to an empty list to disable. Points containing tags for which there is no column will be skipped.
#   # tag_table_add_column_templates = [
#   #   '''ALTER TABLE {{ .table }} ADD COLUMN IF NOT EXISTS {{ .columns|join ", ADD COLUMN IF NOT EXISTS " }}''', ]
#
#   ## The postgres data type to use for storing unsigned 64-bit integer values (Postgres does not have a native unsigned 64-bit integer type). The value can be one of:
#   ##   numeric - Uses the PostgreSQL "numeric" data type. uint8 - Requires pguint extension (https://github.com/petere/pguint)
#   # uint64_type = "numeric"
#
#   ## When using pool_max_conns>1, and a temporary error occurs, the query is retried with an incremental backoff. This controls the maximum backoff duration.
#   # retry_max_backoff = "15s"
#
#   ## Approximate number of tag IDs to store in in-memory cache (when using tags_as_foreign_keys). This is an optimization to skip inserting known tag IDs. Each entry consumes approximately 34 
#   ## bytes of memory.
#   # tag_cache_size = 100000
#
#   ## Enable & set the log level for the Postgres driver.
#   # log_level = "warn" # trace, debug, info, warn, error, none
# # Configuration for the Prometheus client to spawn
[[outputs.prometheus_client]]
#   ## Address to listen on.
   listen = ":9273"
#
#   ## Maximum duration before timing out read of the request
#   # read_timeout = "10s"
#   ## Maximum duration before timing out write of the response
#   # write_timeout = "10s"
#
#   ## Metric version controls the mapping from Prometheus metrics into Telegraf metrics. See "Metric Format Configuration" in plugins/inputs/prometheus/README.md for details. Valid options: 1, 2
    metric_version = 2
#
#   ## Use HTTP Basic Authentication.
#   # basic_username = "Foo" basic_password = "Bar"
#
#   ## If set, the IP Ranges which are allowed to access metrics.
#   ##   ex: ip_range = ["192.168.0.0/24", "192.168.1.0/30"]
#   # ip_range = []
#
#   ## Path to publish the metrics on.
    path = "/metrics"
#
#   ## Expiration interval for each metric. 0 == no expiration
    expiration_interval = "60s"
#
#   ## Collectors to enable, valid entries are "gocollector" and "process". If unset, both are enabled.
#   # collectors_exclude = ["gocollector", "process"]
#
#   ## Send string metrics as Prometheus labels. Unless set to false all string metrics will be sent as labels.
#   # string_as_label = true
#
#   ## If set, enable TLS with the given certificate.
#   # tls_cert = "/etc/ssl/telegraf.crt" tls_key = "/etc/ssl/telegraf.key"
#
#   ## Set one or more allowed client CA certificate file names to enable mutually authenticated TLS connections
#   # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## Export metric collection time.
#   # export_timestamp = false
# # Publishes metrics to a redis timeseries server
# [[outputs.redistimeseries]]
#   ## The address of the RedisTimeSeries server.
#   address = "127.0.0.1:6379"
#
#   ## Redis ACL credentials
#   # username = "" password = "" database = 0
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem" insecure_skip_verify = false
# # Configuration for Riemann to send metrics to
# [[outputs.riemann]]
#   ## The full TCP or UDP URL of the Riemann server
#   url = "tcp://localhost:5555"
#
#   ## Riemann event TTL, floating-point time in seconds. Defines how long that an event is considered valid for in Riemann
#   # ttl = 30.0
#
#   ## Separator to use between measurement and field name in Riemann service name This does not have any effect if 'measurement_as_attribute' is set to 'true'
#   separator = "/"
#
#   ## Set measurement name as Riemann attribute 'measurement', instead of prepending it to the Riemann service name
#   # measurement_as_attribute = false
#
#   ## Send string metrics as Riemann event states. Unless enabled all string metrics will be ignored
#   # string_as_state = false
#
#   ## A list of tag keys whose values get sent as Riemann tags. If empty, all Telegraf tag values will be sent as tags
#   # tag_keys = ["telegraf","custom_tag"]
#
#   ## Additional Riemann tags to send.
#   # tags = ["telegraf-output"]
#
#   ## Description for Riemann event
#   # description_text = "metrics collected from telegraf"
#
#   ## Riemann client write timeout, defaults to "5s" if not set.
#   # timeout = "5s"
#  ## DEPRECATED: The "riemann_legacy" plugin is deprecated in version 1.3.0, use 'outputs.riemann' instead (see https://github.com/influxdata/telegraf/issues/1878).
# # Configuration for the Riemann server to send metrics to
# [[outputs.riemann_legacy]]
#   ## URL of server
#   url = "localhost:5555"
#   ## transport protocol to use either tcp or udp
#   transport = "tcp"
#   ## separator to use between input name and field name in Riemann service name
#   separator = " "
# # Send aggregate metrics to Sensu Monitor
# [[outputs.sensu]]
#   ## BACKEND API URL is the Sensu Backend API root URL to send metrics to (protocol, host, and port only). The output plugin will automatically append the corresponding backend API path 
#   ## /api/core/v2/namespaces/:entity_namespace/events/:entity_name/:check_name).
#   ##
#   ## Backend Events API reference: https://docs.sensu.io/sensu-go/latest/api/events/
#   ##
#   ## AGENT API URL is the Sensu Agent API root URL to send metrics to (protocol, host, and port only). The output plugin will automatically append the correspeonding agent API path (/events).
#   ##
#   ## Agent API Events API reference: https://docs.sensu.io/sensu-go/latest/api/events/
#   ##
#   ## NOTE: if backend_api_url and agent_api_url and api_key are set, the output plugin will use backend_api_url. If backend_api_url and agent_api_url are not provided, the output plugin will 
#   ## default to use an agent_api_url of http://127.0.0.1:3031
#   ##
#   # backend_api_url = "http://127.0.0.1:8080" agent_api_url = "http://127.0.0.1:3031"
#
#   ## API KEY is the Sensu Backend API token Generate a new API token via:
#   ##
#   ## $ sensuctl cluster-role create telegraf --verb create --resource events,entities $ sensuctl cluster-role-binding create telegraf --cluster-role telegraf --group telegraf $ sensuctl user 
#   ## create telegraf --group telegraf --password REDACTED $ sensuctl api-key grant telegraf
#   ##
#   ## For more information on Sensu RBAC profiles & API tokens, please visit: - https://docs.sensu.io/sensu-go/latest/reference/rbac/ - https://docs.sensu.io/sensu-go/latest/reference/apikeys/
#   ##
#   # api_key = "${SENSU_API_KEY}"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Timeout for HTTP message
#   # timeout = "5s"
#
#   ## HTTP Content-Encoding for write request body, can be set to "gzip" to compress body or "identity" to apply no encoding.
#   # content_encoding = "identity"
#
#   ## Sensu Event details
#   ##
#   ## Below are the event details to be sent to Sensu.  The main portions of the event are the check, entity, and metrics specifications. For more information on Sensu events and its components, 
#   ## please visit: - Events - https://docs.sensu.io/sensu-go/latest/reference/events - Checks - https://docs.sensu.io/sensu-go/latest/reference/checks - Entities - 
#   ## https://docs.sensu.io/sensu-go/latest/reference/entities - Metrics - https://docs.sensu.io/sensu-go/latest/reference/events#metrics
#   ##
#   ## Check specification The check name is the name to give the Sensu check associated with the event created. This maps to check.metatadata.name in the event.
#   [outputs.sensu.check]
#     name = "telegraf"
#
#   ## Entity specification Configure the entity name and namespace, if necessary. This will be part of the entity.metadata in the event.
#   ##
#   ## NOTE: if the output plugin is configured to send events to a backend_api_url and entity_name is not set, the value returned by os.Hostname() will be used; if the output plugin is configured 
#   ## to send events to an agent_api_url, entity_name and entity_namespace are not used.
#   # [outputs.sensu.entity]
#   #   name = "server-01" namespace = "default"
#
#   ## Metrics specification Configure the tags for the metrics that are sent as part of the Sensu event
#   # [outputs.sensu.tags]
#   #   source = "telegraf"
#
#   ## Configure the handler(s) for processing the provided metrics
#   # [outputs.sensu.metrics]
#   #   handlers = ["influxdb","elasticsearch"]
# # Send metrics and events to SignalFx
# [[outputs.signalfx]]
#   ## SignalFx Org Access Token
#   access_token = "my-secret-token"
#
#   ## The SignalFx realm that your organization resides in
#   signalfx_realm = "us9" # Required if ingest_url is not set
#
#   ## You can optionally provide a custom ingest url instead of the signalfx_realm option above if you are using a gateway or proxy instance.  This option takes precident over signalfx_realm.
#   ingest_url = "https://my-custom-ingest/"
#
#   ## Event typed metrics are omitted by default, If you require an event typed metric you must specify the metric name in the following list.
#   included_event_names = ["plugin.metric_name"]
# # Generic socket writer capable of handling multiple socket types.
# [[outputs.socket_writer]]
#   ## URL to connect to
#   # address = "tcp://127.0.0.1:8094" address = "tcp://example.com:http" address = "tcp4://127.0.0.1:8094" address = "tcp6://127.0.0.1:8094" address = "tcp6://[2001:db8::1]:8094" address = 
#   # "udp://127.0.0.1:8094" address = "udp4://127.0.0.1:8094" address = "udp6://127.0.0.1:8094" address = "unix:///tmp/telegraf.sock" address = "unixgram:///tmp/telegraf.sock"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Period between keep alive probes. Only applies to TCP sockets. 0 disables keep alive probes. Defaults to the OS configuration.
#   # keep_alive_period = "5m"
#
#   ## Content encoding for message payloads, can be set to "gzip" or to "identity" to apply no encoding.
#   ##
#   # content_encoding = "identity"
#
#   ## Data format to generate. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   # data_format = "influx"
# # Save metrics to an SQL Database
# [[outputs.sql]]
#   ## Database driver Valid options: mssql (Microsoft SQL Server), mysql (MySQL), pgx (Postgres),
#   ##  sqlite (SQLite3), snowflake (snowflake.com) clickhouse (ClickHouse)
#   # driver = ""
#
#   ## Data source name The format of the data source name is different for each database driver. See the plugin readme for details.
#   # data_source_name = ""
#
#   ## Timestamp column name
#   # timestamp_column = "timestamp"
#
#   ## Table creation template Available template variables:
#   ##  {TABLE} - table name as a quoted identifier {TABLELITERAL} - table name as a quoted string literal {COLUMNS} - column definitions (list of quoted identifiers and types)
#   # table_template = "CREATE TABLE {TABLE}({COLUMNS})"
#
#   ## Table existence check template Available template variables:
#   ##  {TABLE} - tablename as a quoted identifier
#   # table_exists_template = "SELECT 1 FROM {TABLE} LIMIT 1"
#
#   ## Initialization SQL
#   # init_sql = ""
#
#   ## Metric type to SQL type conversion The values on the left are the data types Telegraf has and the values on the right are the data types Telegraf will use when sending to a database.
#   ##
#   ## The database values used must be data types the destination database understands. It is up to the user to ensure that the selected data type is available in the database they are using. Refer 
#   ## to your database documentation for what data types are available and supported.
#   #[outputs.sql.convert]
#   #  integer = "INT" real = "DOUBLE" text = "TEXT" timestamp = "TIMESTAMP" defaultvalue = "TEXT" unsigned = "UNSIGNED" bool = "BOOL"
#
#   ## This setting controls the behavior of the unsigned value. By default the setting will take the integer value and append the unsigned value to it. The other option is "literal", which will use 
#   ## the actual value the user provides to the unsigned option. This is useful for a database like ClickHouse where the unsigned value should use a value like "uint64".
#   # conversion_style = "unsigned_suffix"
#
#   ## Maximum amount of time a connection may be idle. "0s" means connections are never closed due to idle time.
#   # connection_max_idle_time = "0s"
#
#   ## Maximum amount of time a connection may be reused. "0s" means connections are never closed due to age.
#   # connection_max_lifetime = "0s"
#
#   ## Maximum number of connections in the idle connection pool. 0 means unlimited.
#   # connection_max_idle = 2
#
#   ## Maximum number of open connections to the database. 0 means unlimited.
#   # connection_max_open = 0
# # Configuration for Google Cloud Stackdriver to send metrics to
# [[outputs.stackdriver]]
#   ## GCP Project
#   project = "erudite-bloom-151019"
#
#   ## The namespace for the metric descriptor
#   namespace = "telegraf"
#
#   ## Custom resource type
#   # resource_type = "generic_node"
#
#   ## Additional resource labels
#   # [outputs.stackdriver.resource_labels]
#   #   node_id = "$HOSTNAME" namespace = "myapp" location = "eu-north0"
# # Configuration for active mq with stomp protocol to send metrics to
# [[outputs.stomp]]
#   host = "localhost:61613"
#
#   ## Queue name for producer messages
#   queueName = "telegraf"
#
#   ## Username and password if required by the Active MQ server.
#   # username = "" password = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#
#   ## Data format to output.
#   data_format = "json"
# # A plugin that can send metrics to Sumo Logic HTTP metric collector.
# [[outputs.sumologic]]
#   ## Unique URL generated for your HTTP Metrics Source. This is the address to send metrics to.
#   # url = "https://events.sumologic.net/receiver/v1/http/<UniqueHTTPCollectorCode>"
#
#   ## Data format to be used for sending metrics. This will set the "Content-Type" header accordingly. Currently supported formats: * graphite - for Content-Type of 
#   ## application/vnd.sumologic.graphite * carbon2 - for Content-Type of application/vnd.sumologic.carbon2 * prometheus - for Content-Type of application/vnd.sumologic.prometheus
#   ##
#   ## More information can be found at: https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#content-type-headers-for-metrics
#   ##
#   ## NOTE: When unset, telegraf will by default use the influx serializer which is currently unsupported in HTTP Source.
#   data_format = "carbon2"
#
#   ## Timeout used for HTTP request
#   # timeout = "5s"
#
#   ## Max HTTP request body size in bytes before compression (if applied). By default 1MB is recommended. NOTE: Bear in mind that in some serializer a metric even though serialized to multiple 
#   ## lines cannot be split any further so setting this very low might not work as expected.
#   # max_request_body_size = 1000000
#
#   ## Additional, Sumo specific options. Full list can be found here: 
#   ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#supported-http-headers
#
#   ## Desired source name. Useful if you want to override the source name configured for the source.
#   # source_name = ""
#
#   ## Desired host name. Useful if you want to override the source host configured for the source.
#   # source_host = ""
#
#   ## Desired source category. Useful if you want to override the source category configured for the source.
#   # source_category = ""
#
#   ## Comma-separated key=value list of dimensions to apply to every metric. Custom dimensions will allow you to query your metrics at a more granular level.
#   # dimensions = ""
# # Configuration for Syslog server to send metrics to
# [[outputs.syslog]]
#   ## URL to connect to ex: address = "tcp://127.0.0.1:8094" ex: address = "tcp4://127.0.0.1:8094" ex: address = "tcp6://127.0.0.1:8094" ex: address = "tcp6://[2001:db8::1]:8094" ex: address = 
#   ## "udp://127.0.0.1:8094" ex: address = "udp4://127.0.0.1:8094" ex: address = "udp6://127.0.0.1:8094"
#   address = "tcp://127.0.0.1:8094"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Period between keep alive probes. Only applies to TCP sockets. 0 disables keep alive probes. Defaults to the OS configuration.
#   # keep_alive_period = "5m"
#
#   ## The framing technique with which it is expected that messages are transported (default = "octet-counting").  Whether the messages come using the octect-counting (RFC5425#section-4.3.1, 
#   ## RFC6587#section-3.4.1), or the non-transparent framing technique (RFC6587#section-3.4.2).  Must be one of "octet-counting", "non-transparent".
#   # framing = "octet-counting"
#
#   ## The trailer to be expected in case of non-transparent framing (default = "LF"). Must be one of "LF", or "NUL".
#   # trailer = "LF"
#
#   ## SD-PARAMs settings Syslog messages can contain key/value pairs within zero or more structured data sections.  For each unrecognized metric tag/field a SD-PARAMS is created.
#   ##
#   ## Example:
#   ##   [[outputs.syslog]]
#   ##     sdparam_separator = "_" default_sdid = "default@32473" sdids = ["foo@123", "bar@456"]
#   ##
#   ##   input => xyzzy,x=y foo@123_value=42,bar@456_value2=84,something_else=1 output (structured data only) => [foo@123 value=42][bar@456 value2=84][default@32473 something_else=1 x=y]
#
#   ## SD-PARAMs separator between the sdid and tag/field key (default = "_")
#   # sdparam_separator = "_"
#
#   ## Default sdid used for tags/fields that don't contain a prefix defined in the explicit sdids setting below If no default is specified, no SD-PARAMs will be used for unrecognized field.
#   # default_sdid = "default@32473"
#
#   ## List of explicit prefixes to extract from tag/field keys and use as the SDID, if they match (see above example for more details):
#   # sdids = ["foo@123", "bar@456"]
#
#   ## Default severity value. Severity and Facility are used to calculate the message PRI value (RFC5424#section-6.2.1).  Used when no metric field with key "severity_code" is defined.  If unset, 5 
#   ## (notice) is the default
#   # default_severity_code = 5
#
#   ## Default facility value. Facility and Severity are used to calculate the message PRI value (RFC5424#section-6.2.1).  Used when no metric field with key "facility_code" is defined.  If unset, 1 
#   ## (user-level) is the default
#   # default_facility_code = 1
#
#   ## Default APP-NAME value (RFC5424#section-6.2.5) Used when no metric tag with key "appname" is defined. If unset, "Telegraf" is the default
#   # default_appname = "Telegraf"
# # Configuration for sending metrics to Amazon Timestream.
# [[outputs.timestream]]
#   ## Amazon Region
#   region = "us-east-1"
#
#   ## Amazon Credentials Credentials are loaded in the following order: 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified 2) Assumed credentials via 
#   ## STS if role_arn is specified 3) explicit credentials from 'access_key' and 'secret_key' 4) shared profile from 'profile' 5) environment variables 6) shared credentials file 7) EC2 Instance 
#   ## Profile
#   #access_key = "" secret_key = "" token = "" role_arn = "" web_identity_token_file = "" role_session_name = "" profile = "" shared_credential_file = ""
#
#   ## Endpoint to make request against, the correct endpoint is automatically determined and this option should only be set if you wish to override the default.
#   ##   ex: endpoint_url = "http://localhost:8000"
#   # endpoint_url = ""
#
#   ## Timestream database where the metrics will be inserted. The database must exist prior to starting Telegraf.
#   database_name = "yourDatabaseNameHere"
#
#   ## Specifies if the plugin should describe the Timestream database upon starting to validate if it has access necessary permissions, connection, etc., as a safety check. If the describe 
#   ## operation fails, the plugin will not start and therefore the Telegraf agent will not start.
#   describe_database_on_start = false
#
#   ## Specifies how the data is organized in Timestream. Valid values are: single-table, multi-table. When mapping_mode is set to single-table, all of the data is stored in a single table. When 
#   ## mapping_mode is set to multi-table, the data is organized and stored in multiple tables. The default is multi-table.
#   mapping_mode = "multi-table"
#
#   ## Specifies if the plugin should create the table, if the table does not exist.
#   create_table_if_not_exists = true
#
#   ## Specifies the Timestream table magnetic store retention period in days. Check Timestream documentation for more details. NOTE: This property is valid when create_table_if_not_exists = true.
#   create_table_magnetic_store_retention_period_in_days = 365
#
#   ## Specifies the Timestream table memory store retention period in hours. Check Timestream documentation for more details. NOTE: This property is valid when create_table_if_not_exists = true.
#   create_table_memory_store_retention_period_in_hours = 24
#
#   ## Specifies how the data is written into Timestream. Valid values are: true, false When use_multi_measure_records is set to true, all of the tags and fields are stored as a single row in a 
#   ## Timestream table. When use_multi_measure_record is set to false, Timestream stores each field in a separate table row, thereby storing the tags multiple times (once for each field). The 
#   ## recommended setting is true. The default is false.
#   use_multi_measure_records = "false"
#
#   ## Specifies the measure_name to use when sending multi-measure records. NOTE: This property is valid when use_multi_measure_records=true and mapping_mode=multi-table
#   measure_name_for_multi_measure_records = "telegraf_measure"
#
#   ## Specifies the name of the table to write data into NOTE: This property is valid when mapping_mode=single-table.
#   # single_table_name = ""
#
#   ## Specifies the name of dimension when all of the data is being stored in a single table and the measurement name is transformed into the dimension value (see Mapping data from Influx to 
#   ## Timestream for details) NOTE: This property is valid when mapping_mode=single-table.
#   # single_table_dimension_name_for_telegraf_measurement_name = "namespace"
#
#   ## Only valid and optional if create_table_if_not_exists = true Specifies the Timestream table tags. Check Timestream documentation for more details
#   # create_table_tags = { "foo" = "bar", "environment" = "dev"}
#
#   ## Specify the maximum number of parallel go routines to ingest/write data If not specified, defaulted to 1 go routines
#   max_write_go_routines = 25
#
#   ## Please see README.md to know how line protocol data is mapped to Timestream
#   ##
# # Write metrics to Warp 10
# [[outputs.warp10]]
#   # Prefix to add to the measurement.
#   prefix = "telegraf."
#
#   # URL of the Warp 10 server
#   warp_url = "http://localhost:8080"
#
#   # Write token to access your app on warp 10
#   token = "Token"
#
#   # Warp 10 query timeout timeout = "15s"
#
#   ## Print Warp 10 error body
#   # print_error_body = false
#
#   ##Â Max string error size
#   # max_string_error_size = 511
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# [[outputs.wavefront]]
#   ## Url for Wavefront API or Wavefront Proxy instance.
#   url = "https://metrics.wavefront.com"
#
#   ## Authentication Token for Wavefront. Required if using Direct Ingestion. Not required if using a Wavefront Proxy.
#   #token = "DUMMY_TOKEN"
#
#   ## Maximum number of metrics to send per HTTP request. This value should be higher than the `metric_batch_size`. Default is 10,000. Values higher than 40,000 are not recommended.
#   # http_maximum_batch_size = 10000
#
#   ## Deprecated. DNS name of the Wavefront server or Wavefront Proxy. Use the `url` field instead.
#   #host = "wavefront.example.com"
#
#   ## Deprecated. Wavefront proxy port. Use the `url` field instead.
#   #port = 2878
#
#   ## prefix for metrics keys
#   #prefix = "my.specific.prefix."
#
#   ## whether to use "value" for name of simple fields. default is false
#   #simple_fields = false
#
#   ## character to use between metric and field name.  default is . (dot)
#   #metric_separator = "."
#
#   ## Convert metric name paths to use metricSeparator character When true will convert all _ (underscore) characters in final metric name. default is true
#   #convert_paths = true
#
#   ## Use Strict rules to sanitize metric and tag names from invalid characters When enabled forward slash (/) and comma (,) will be accepted
#   #use_strict = false
#
#   ## Use Regex to sanitize metric and tag names from invalid characters Regex is more thorough, but significantly slower. default is false
#   #use_regex = false
#
#   ## point tags to use as the source name for Wavefront (if none found, host will be used)
#   #source_override = ["hostname", "address", "agent_host", "node_host"]
#
#   ## whether to convert boolean values to numeric values, with false -> 0.0 and true -> 1.0. default is true
#   #convert_bool = true
#
#   ## Truncate metric tags to a total of 254 characters for the tag name value. Wavefront will reject any data point exceeding this limit if not truncated. Defaults to 'false' to provide backwards 
#   ## compatibility.
#   #truncate_tags = false
#
#   ## Flush the internal buffers after each batch. This effectively bypasses the background sending of metrics normally done by the Wavefront SDK. This can be used if you are experiencing buffer 
#   ## overruns. The sending of metrics will block for a longer time, but this will be handled gracefully by the internal buffering in Telegraf.
#   #immediate_flush = true
# # A plugin that can transmit metrics over WebSocket.
# [[outputs.websocket]]
#   ## URL is the address to send metrics to. Make sure ws or wss scheme is used.
#   url = "ws://127.0.0.1:3000/telegraf"
#
#   ## Timeouts (make sure read_timeout is larger than server ping interval or set to zero).
#   # connect_timeout = "30s" write_timeout = "30s" read_timeout = "30s"
#
#   ## Optionally turn on using text data frames (binary by default).
#   # use_text_frames = false
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional SOCKS5 proxy to use
#   # socks5_enabled = true socks5_address = "127.0.0.1:1080" socks5_username = "alice" socks5_password = "pass123"
#
#   ## Optional HTTP proxy to use
#   # use_system_proxy = false http_proxy_url = "http://localhost:8888"
#
#   ## Data format to output. Each data format has it's own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   # data_format = "influx"
#
#   ## Additional HTTP Upgrade headers
#   # [outputs.websocket.headers]
#   #   Authorization = "Bearer <TOKEN>"
# # Send aggregated metrics to Yandex.Cloud Monitoring
# [[outputs.yandex_cloud_monitoring]]
#   ## Timeout for HTTP writes.
#   # timeout = "20s"
#
#   ## Yandex.Cloud monitoring API endpoint. Normally should not be changed
#   # endpoint_url = "https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write"
#
#   ## All user metrics should be sent with "custom" service specified. Normally should not be changed
#   # service = "custom"
###############################################################################
#                            PROCESSOR PLUGINS #
###############################################################################
# # Attach AWS EC2 metadata to metrics
# [[processors.aws_ec2]]
#   ## Instance identity document tags to attach to metrics. For more information see: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html
#   ##
#   ## Available tags: * accountId * architecture * availabilityZone * billingProducts * imageId * instanceId * instanceType * kernelId * pendingTime * privateIp * ramdiskId * region * version
#   imds_tags = []
#
#   ## EC2 instance tags retrieved with DescribeTags action. In case tag is empty upon retrieval it's omitted when tagging metrics. Note that in order for this to work, role attached to EC2 instance 
#   ## or AWS credentials available from the environment must have a policy attached, that allows ec2:DescribeTags.
#   ##
#   ## For more information see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeTags.html
#   ec2_tags = []
#
#   ## Timeout for http requests made by against aws ec2 metadata endpoint.
#   timeout = "10s"
#
#   ## ordered controls whether or not the metrics need to stay in the same order this plugin received them in. If false, this plugin will change the order with requests hitting cached results 
#   ## moving through immediately and not waiting on slower lookups. This may cause issues for you if you are depending on the order of metrics staying the same. If so, set this to true. Keeping the 
#   ## metrics ordered may be slightly slower.
#   ordered = false
#
#   ## max_parallel_calls is the maximum number of AWS API calls to be in flight at the same time. It's probably best to keep this number fairly low.
#   max_parallel_calls = 10
# # Apply metric modifications using override semantics.
# [[processors.clone]]
#   ## All modifications on inputs and aggregators can be overridden:
#   # name_override = "new_name" name_prefix = "new_name_prefix" name_suffix = "new_name_suffix"
#
#   ## Tags to be added (all values must be strings)
#   # [processors.clone.tags]
#   #   additional_tag = "tag_value"
# # Convert values to another metric value type
# [[processors.converter]]
#   ## Tags to convert
#   ##
#   ## The table key determines the target type, and the array of key-values select the keys to convert.  The array may contain globs.
#   ##   <target-type> = [<tag-key>...]
#   [processors.converter.tags]
#     measurement = [] string = [] integer = [] unsigned = [] boolean = [] float = []
#
#     ## Optional tag to use as metric timestamp
#     # timestamp = []
#
#     ## Format of the timestamp determined by the tag above. This can be any of "unix", "unix_ms", "unix_us", "unix_ns", or a valid Golang time format. It is required, when using the timestamp 
#     ## option.
#     # timestamp_format = ""
#
#   ## Fields to convert
#   ##
#   ## The table key determines the target type, and the array of key-values select the keys to convert.  The array may contain globs.
#   ##   <target-type> = [<field-key>...]
#   [processors.converter.fields]
#     measurement = [] tag = [] string = [] integer = [] unsigned = [] boolean = [] float = []
#
#     ## Optional field to use as metric timestamp
#     # timestamp = []
#
#     ## Format of the timestamp determined by the field above. This can be any of "unix", "unix_ms", "unix_us", "unix_ns", or a valid Golang time format. It is required, when using the timestamp 
#     ## option.
#     # timestamp_format = ""
# # Dates measurements, tags, and fields that pass through this filter.
# [[processors.date]]
#   ## New tag to create
#   tag_key = "month"
#
#   ## New field to create (cannot set both field_key and tag_key)
#   # field_key = "month"
#
#   ## Date format string, must be a representation of the Go "reference time" which is "Mon Jan 2 15:04:05 -0700 MST 2006".
#   date_format = "Jan"
#
#   ## If destination is a field, date format can also be one of "unix", "unix_ms", "unix_us", or "unix_ns", which will insert an integer field.
#   # date_format = "unix"
#
#   ## Offset duration added to the date string when writing the new tag.
#   # date_offset = "0s"
#
#   ## Timezone to use when creating the tag or field using a reference time string.  This can be set to one of "UTC", "Local", or to a location name in the IANA Time Zone database.
#   ##   example: timezone = "America/Los_Angeles"
#   # timezone = "UTC"
# # Filter metrics with repeating field values
# [[processors.dedup]]
#   ## Maximum time to suppress output
#   dedup_interval = "600s"
# ## Set default fields on your metric(s) when they are nil or empty
# [[processors.defaults]]
#   ## Ensures a set of fields always exists on your metric(s) with their respective default value. For any given field pair (key = default), if it's not set, a field is set on the metric with the 
#   ## specified default.
#   ##
#   ## A field is considered not set if it is nil on the incoming metric; or it is not nil but its value is an empty string or is a string of one or more spaces.
#   ##   <target-field> = <value>
#   [processors.defaults.fields]
#     field_1 = "bar" time_idle = 0 is_error = true
# # Map enum values according to given table.
# [[processors.enum]]
#   [[processors.enum.mapping]]
#     ## Name of the field to map. Globs accepted.
#     field = "status"
#
#     ## Name of the tag to map. Globs accepted.
#     # tag = "status"
#
#     ## Destination tag or field to be used for the mapped value.  By default the source tag or field is used, overwriting the original value.
#     dest = "status_code"
#
#     ## Default value to be used for all values not contained in the mapping table.  When unset and no match is found, the original field will remain unmodified and the destination tag or field 
#     ## will not be created.
#     # default = 0
#
#     ## Table of mappings
#     [processors.enum.mapping.value_mappings]
#       green = 1 amber = 2 red = 3
# # Run executable as long-running processor plugin
# [[processors.execd]]
#   ## One program to run as daemon. NOTE: process and each argument should each be their own string eg: command = ["/path/to/your_program", "arg1", "arg2"]
#   command = ["cat"]
#
#   ## Environment variables Array of "key=value" pairs to pass as environment variables e.g. "KEY=value", "USERNAME=John Doe", "LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs"
#   # environment = []
#
#   ## Delay before the process is restarted after an unexpected termination
#   # restart_delay = "10s"
# # Performs file path manipulations on tags and fields
# [[processors.filepath]]
#   ## Treat the tag value as a path and convert it to its last element, storing the result in a new tag
#   # [[processors.filepath.basename]]
#   #   tag = "path" dest = "basepath"
#
#   ## Treat the field value as a path and keep all but the last element of path, typically the path's directory
#   # [[processors.filepath.dirname]]
#   #   field = "path"
#
#   ## Treat the tag value as a path, converting it to its the last element without its suffix
#   # [[processors.filepath.stem]]
#   #   tag = "path"
#
#   ## Treat the tag value as a path, converting it to the shortest path name equivalent to path by purely lexical processing
#   # [[processors.filepath.clean]]
#   #   tag = "path"
#
#   ## Treat the tag value as a path, converting it to a relative path that is lexically equivalent to the source path when joined to 'base_path'
#   # [[processors.filepath.rel]]
#   #   tag = "path" base_path = "/var/log"
#
#   ## Treat the tag value as a path, replacing each separator character in path with a '/' character. Has only effect on Windows
#   # [[processors.filepath.toslash]]
#   #   tag = "path"
# # Add a tag of the network interface name looked up over SNMP by interface number
# [[processors.ifname]]
#   ## Name of tag holding the interface number
#   # tag = "ifIndex"
#
#   ## Name of output tag where service name will be added
#   # dest = "ifName"
#
#   ## Name of tag of the SNMP agent to request the interface name from
#   # agent = "agent"
#
#   ## Timeout for each request.
#   # timeout = "5s"
#
#   ## SNMP version; can be 1, 2, or 3.
#   # version = 2
#
#   ## SNMP community string.
#   # community = "public"
#
#   ## Number of retries to attempt.
#   # retries = 3
#
#   ## The GETBULK max-repetitions parameter.
#   # max_repetitions = 10
#
#   ## SNMPv3 authentication and encryption options.
#   ##
#   ## Security Name.
#   # sec_name = "myuser"
#   ## Authentication protocol; one of "MD5", "SHA", or "".
#   # auth_protocol = "MD5"
#   ## Authentication password.
#   # auth_password = "pass"
#   ## Security Level; one of "noAuthNoPriv", "authNoPriv", or "authPriv".
#   # sec_level = "authNoPriv"
#   ## Context Name.
#   # context_name = ""
#   ## Privacy protocol used for encrypted messages; one of "DES", "AES" or "".
#   # priv_protocol = ""
#   ## Privacy password used for encrypted messages.
#   # priv_password = ""
#
#   ## max_parallel_lookups is the maximum number of SNMP requests to make at the same time.
#   # max_parallel_lookups = 100
#
#   ## ordered controls whether or not the metrics need to stay in the same order this plugin received them in. If false, this plugin may change the order when data is cached.  If you need metrics 
#   ## to stay in order set this to true.  keeping the metrics ordered may be slightly slower
#   # ordered = false
#
#   ## cache_ttl is the amount of time interface names are cached for a given agent.  After this period elapses if names are needed they will be retrieved again.
#   # cache_ttl = "8h"
# # Lookup a key derived from metrics in a static file
# [[processors.lookup]]
#   ## List of files containing the lookup-table
#   files = ["path/to/lut.json", "path/to/another_lut.json"]
#
#   ## Format of the lookup file(s) Available formats are:
#   ##    json -- JSON file with 'key: {tag-key: tag-value, ...}' mapping csv_key_name_value -- CSV file with 'key,tag-key,tag-value,...,tag-key,tag-value' mapping csv_key_values -- CSV file with a 
#   ##    header containing tag-names and
#   ##                          rows with 'key,tag-value,...,tag-value' mappings
#   # format = "json"
#
#   ## Template for generating the lookup-key from the metric. This is a Golang template (see https://pkg.go.dev/text/template) to access the metric name (`{{.Name}}`), a tag value (`{{.Tag 
#   ## "name"}}`) or a field value (`{{.Field "name"}}`).
#   key = '{{.Tag "host"}}'
# # Adds noise to numerical fields
# [[processors.noise]]
#   ## Specified the type of the random distribution. Can be "laplacian", "gaussian" or "uniform".
#   # type = "laplacian
#
#   ## Center of the distribution. Only used for Laplacian and Gaussian distributions.
#   # mu = 0.0
#
#   ## Scale parameter for the Laplacian or Gaussian distribution
#   # scale = 1.0
#
#   ## Upper and lower bound of the Uniform distribution
#   # min = -1.0 max = 1.0
#
#   ## Apply the noise only to numeric fields matching the filter criteria below. Excludes takes precedence over includes.
#   # include_fields = [] exclude_fields = []
# # Apply metric modifications using override semantics.
# [[processors.override]]
#   ## All modifications on inputs and aggregators can be overridden:
#   # name_override = "new_name" name_prefix = "new_name_prefix" name_suffix = "new_name_suffix"
#
#   ## Tags to be added (all values must be strings)
#   # [processors.override.tags]
#   #   additional_tag = "tag_value"
# # Parse a value in a specified field(s)/tag(s) and add the result in a new metric
# [[processors.parser]]
#   ## The name of the fields whose value will be parsed.
#   parse_fields = ["message"]
#
#   ## The name of the tags whose value will be parsed.
#   # parse_tags = []
#
#   ## If true, incoming metrics are not emitted.
#   # drop_original = false
#
#   ## If set to override, emitted metrics will be merged by overriding the original metric using the newly parsed metrics. Only has effect when drop_original is set to false.
#   merge = "override"
#
#   ## The dataformat to be read from files Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"
# # Rotate a single valued metric into a multi field metric
# [[processors.pivot]]
#   ## Tag to use for naming the new field.
#   tag_key = "name"
#   ## Field to use as the value of the new field.
#   value_key = "value"
# # Given a tag/field of a TCP or UDP port number, add a tag/field of the service name looked up in the system services file
# [[processors.port_name]]
#   ## Name of tag holding the port number
#   # tag = "port"
#   ## Or name of the field holding the port number
#   # field = "port"
#
#   ## Name of output tag or field (depending on the source) where service name will be added
#   # dest = "service"
#
#   ## Default tcp or udp
#   # default_protocol = "tcp"
#
#   ## Tag containing the protocol (tcp or udp, case-insensitive)
#   # protocol_tag = "proto"
#
#   ## Field containing the protocol (tcp or udp, case-insensitive)
#   # protocol_field = "proto"
# # Print all metrics that pass through this filter.
# [[processors.printer]]
# # Transforms tag and field values as well as measurement, tag and field names with regex pattern
# [[processors.regex]]
#   namepass = ["nginx_requests"]
#
#   # Tag and field conversions defined in a separate sub-tables
#   [[processors.regex.tags]]
#     ## Tag to change, "*" will change every tag
#     key = "resp_code"
#     ## Regular expression to match on a tag value
#     pattern = "^(\\d)\\d\\d$"
#     ## Matches of the pattern will be replaced with this string.  Use ${1} notation to use the text of the first submatch.
#     replacement = "${1}xx"
#
#   [[processors.regex.fields]]
#     ## Field to change
#     key = "request"
#     ## All the power of the Go regular expressions available here For example, named subgroups
#     pattern = "^/api(?P<method>/[\\w/]+)\\S*" replacement = "${method}"
#     ## If result_key is present, a new field will be created instead of changing existing field
#     result_key = "method"
#
#   # Multiple conversions may be applied for one field sequentially Let's extract one more value
#   [[processors.regex.fields]]
#     key = "request" pattern = ".*category=(\\w+).*" replacement = "${1}" result_key = "search_category"
#
#   # Rename metric fields
#   [[processors.regex.field_rename]]
#     ## Regular expression to match on a field name
#     pattern = "^search_(\\w+)d$"
#     ## Matches of the pattern will be replaced with this string.  Use ${1} notation to use the text of the first submatch.
#     replacement = "${1}"
#     ## If the new field name already exists, you can either "overwrite" the existing one with the value of the renamed field OR you can "keep" both the existing and source field.
#     # result_key = "keep"
#
#   # Rename metric tags [[processors.regex.tag_rename]]
#   #   ## Regular expression to match on a tag name
#   #   pattern = "^search_(\\w+)d$"
#   #   ## Matches of the pattern will be replaced with this string.  Use ${1} notation to use the text of the first submatch.
#   #   replacement = "${1}"
#   #   ## If the new tag name already exists, you can either "overwrite" the existing one with the value of the renamed tag OR you can "keep" both the existing and source tag.
#   #   # result_key = "keep"
#
#   # Rename metrics [[processors.regex.metric_rename]]
#   #   ## Regular expression to match on an metric name
#   #   pattern = "^search_(\\w+)d$"
#   #   ## Matches of the pattern will be replaced with this string.  Use ${1} notation to use the text of the first submatch.
#   #   replacement = "${1}"
# # Rename measurements, tags, and fields that pass through this filter.
# [[processors.rename]]
#   ## Specify one sub-table per rename operation.
#   [[processors.rename.replace]]
#     measurement = "network_interface_throughput" dest = "throughput"
#
#   [[processors.rename.replace]]
#     tag = "hostname" dest = "host"
#
#   [[processors.rename.replace]]
#     field = "lower" dest = "min"
#
#   [[processors.rename.replace]]
#     field = "upper" dest = "max"
# # ReverseDNS does a reverse lookup on IP addresses to retrieve the DNS name
# [[processors.reverse_dns]]
#   ## For optimal performance, you may want to limit which metrics are passed to this processor. eg: namepass = ["my_metric_*"]
#
#   ## cache_ttl is how long the dns entries should stay cached for. generally longer is better, but if you expect a large number of diverse lookups you'll want to consider memory use.
#   cache_ttl = "24h"
#
#   ## lookup_timeout is how long should you wait for a single dns request to repsond. this is also the maximum acceptable latency for a metric travelling through the reverse_dns processor. After 
#   ## lookup_timeout is exceeded, a metric will be passed on unaltered. multiple simultaneous resolution requests for the same IP will only make a single rDNS request, and they will all wait for 
#   ## the answer for this long.
#   lookup_timeout = "3s"
#
#   ## max_parallel_lookups is the maximum number of dns requests to be in flight at the same time. Requesting hitting cached values do not count against this total, and neither do mulptiple 
#   ## requests for the same IP. It's probably best to keep this number fairly low.
#   max_parallel_lookups = 10
#
#   ## ordered controls whether or not the metrics need to stay in the same order this plugin received them in. If false, this plugin will change the order with requests hitting cached results 
#   ## moving through immediately and not waiting on slower lookups. This may cause issues for you if you are depending on the order of metrics staying the same. If so, set this to true. keeping the 
#   ## metrics ordered may be slightly slower.
#   ordered = false
#
#   [[processors.reverse_dns.lookup]]
#     ## get the ip from the field "source_ip", and put the result in the field "source_name"
#     field = "source_ip" dest = "source_name"
#
#   [[processors.reverse_dns.lookup]]
#     ## get the ip from the tag "destination_ip", and put the result in the tag "destination_name".
#     tag = "destination_ip" dest = "destination_name"
#
#     ## If you would prefer destination_name to be a field instead, you can use a processors.converter after this one, specifying the order attribute.
# # Add the S2 Cell ID as a tag based on latitude and longitude fields
# [[processors.s2geo]]
#   ## The name of the lat and lon fields containing WGS-84 latitude and longitude in decimal degrees.
#   # lat_field = "lat" lon_field = "lon"
#
#   ## New tag to create
#   # tag_key = "s2_cell_id"
#
#   ## Cell level (see https://s2geometry.io/resources/s2cell_statistics.html)
#   # cell_level = 9
# # Process metrics using a Starlark script
# [[processors.starlark]]
#   ## The Starlark source can be set as a string in this configuration file, or by referencing a file containing the script.  Only one source or script should be set at once.
#
#   ## Source of the Starlark script.
#   source = ''' def apply(metric): return metric '''
#
#   ## File containing a Starlark script.
#   # script = "/usr/local/bin/myscript.star"
#
#   ## The constants of the Starlark script.
#   # [processors.starlark.constants]
#   #   max_size = 10 threshold = 0.75 default_name = "Julia" debug_mode = true
# # Perform string processing on tags, fields, and measurements
# [[processors.strings]]
#   ## Convert a field value to lowercase and store in a new field
#   # [[processors.strings.lowercase]]
#   #   field = "uri_stem" dest = "uri_stem_normalised"
#
#   ## Convert a tag value to uppercase
#   # [[processors.strings.uppercase]]
#   #   tag = "method"
#
#   ## Convert a field value to titlecase
#   # [[processors.strings.titlecase]]
#   #   field = "status"
#
#   ## Trim leading and trailing whitespace using the default cutset
#   # [[processors.strings.trim]]
#   #   field = "message"
#
#   ## Trim leading characters in cutset
#   # [[processors.strings.trim_left]]
#   #   field = "message" cutset = "\t"
#
#   ## Trim trailing characters in cutset
#   # [[processors.strings.trim_right]]
#   #   field = "message" cutset = "\r\n"
#
#   ## Trim the given prefix from the field
#   # [[processors.strings.trim_prefix]]
#   #   field = "my_value" prefix = "my_"
#
#   ## Trim the given suffix from the field
#   # [[processors.strings.trim_suffix]]
#   #   field = "read_count" suffix = "_count"
#
#   ## Replace all non-overlapping instances of old with new
#   # [[processors.strings.replace]]
#   #   measurement = "*" old = ":" new = "_"
#
#   ## Trims strings based on width
#   # [[processors.strings.left]]
#   #   field = "message" width = 10
#
#   ## Decode a base64 encoded utf-8 string
#   # [[processors.strings.base64decode]]
#   #   field = "message"
#
#   ## Sanitize a string to ensure it is a valid utf-8 string Each run of invalid UTF-8 byte sequences is replaced by the replacement string, which may be empty
#   # [[processors.strings.valid_utf8]]
#   #   field = "message" replacement = ""
# # Restricts the number of tags that can pass through this filter and chooses which tags to preserve when over the limit.
# [[processors.tag_limit]]
#   ## Maximum number of tags to preserve
#   limit = 3
#
#   ## List of tags to preferentially preserve
#   keep = ["environment", "region"]
# # Uses a Go template to create a new tag
# [[processors.template]]
#   ## Tag to set with the output of the template.
#   tag = "topic"
#
#   ## Go template used to create the tag value.  In order to ease TOML escaping requirements, you may wish to use single quotes around the template string.
#   template = '{{ .Tag "hostname" }}.{{ .Tag "level" }}'
# # Print all metrics that pass through this filter.
# [[processors.topk]]
#   ## How many seconds between aggregations
#   # period = 10
#
#   ## How many top buckets to return per field Every field specified to aggregate over will return k number of results. For example, 1 field with k of 10 will return 10 buckets. While 2 fields with 
#   ## k of 3 will return 6 buckets.
#   # k = 10
#
#   ## Over which tags should the aggregation be done. Globs can be specified, in which case any tag matching the glob will aggregated over. If set to an empty list is no aggregation over tags is 
#   ## done
#   # group_by = ['*']
#
#   ## The field(s) to aggregate Each field defined is used to create an independent aggregation. Each aggregation will return k buckets. If a metric does not have a defined field the metric will be 
#   ## dropped from the aggregation. Considering using the defaults processor plugin to ensure fields are set if required.
#   # fields = ["value"]
#
#   ## What aggregation function to use. Options: sum, mean, min, max
#   # aggregation = "mean"
#
#   ## Instead of the top k largest metrics, return the bottom k lowest metrics
#   # bottomk = false
#
#   ## The plugin assigns each metric a GroupBy tag generated from its name and tags. If this setting is different than "" the plugin will add a tag (which name will be the value of this setting) to 
#   ## each metric with the value of the calculated GroupBy tag. Useful for debugging
#   # add_groupby_tag = ""
#
#   ## These settings provide a way to know the position of each metric in the top k. The 'add_rank_field' setting allows to specify for which fields the position is required. If the list is non 
#   ## empty, then a field will be added to each and every metric for each string present in this setting. This field will contain the ranking of the group that the metric belonged to when 
#   ## aggregated over that field. The name of the field will be set to the name of the aggregation field, suffixed with the string '_topk_rank'
#   # add_rank_fields = []
#
#   ## These settings provide a way to know what values the plugin is generating when aggregating metrics. The 'add_aggregate_field' setting allows to specify for which fields the final aggregation 
#   ## value is required. If the list is non empty, then a field will be added to each every metric for each field present in this setting. This field will contain the computed aggregation for the 
#   ## group that the metric belonged to when aggregated over that field. The name of the field will be set to the name of the aggregation field, suffixed with the string '_topk_aggregate'
#   # add_aggregate_fields = []
# # Rotate multi field metric into several single field metrics
# [[processors.unpivot]]
#   ## Metric mode to pivot to Set to "tag", metrics are pivoted as a tag and the metric is kept as the original measurement name. Tag key name is set by tag_key value. Set to "metric" creates a new 
#   ## metric named the field name. With this option the tag_key is ignored. Be aware that this could lead to metric name conflicts!
#   # use_fieldname_as = "tag"
#
#   ## Tag to use for the name.
#   # tag_key = "name"
#
#   ## Field to use for the name of the value.
#   # value_key = "value"
###############################################################################
#                            AGGREGATOR PLUGINS #
###############################################################################
# # Keep the aggregate basicstats of each metric passing through.
# [[aggregators.basicstats]]
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#
#   ## If true, the original metric will be dropped by the aggregator and will not get sent to the output plugins.
#   drop_original = false
#
#   ## Configures which basic stats to push as fields
#   # stats = ["count","diff","rate","min","max","mean","non_negative_diff","non_negative_rate","stdev","s2","sum","interval"]
# # Calculates a derivative for every field.
# [[aggregators.derivative]]
#   ## The period in which to flush the aggregator.
#   period = "30s"
#   ##
#   ## Suffix to append for the resulting derivative field.
#   # suffix = "_rate"
#   ##
#   ## Field to use for the quotient when computing the derivative. When using a field as the derivation parameter the name of that field will be used for the resulting derivative, e.g. 
#   ## *fieldname_by_parameter*. By default the timestamps of the metrics are used and the suffix is omitted.
#   # variable = ""
#   ##
#   ## Maximum number of roll-overs in case only one measurement is found during a period.
#   # max_roll_over = 10
# # Report the final metric of a series
# [[aggregators.final]]
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#   ## If true, the original metric will be dropped by the aggregator and will not get sent to the output plugins.
#   drop_original = false
#
#   ## The time that a series is not updated until considering it final.
#   series_timeout = "5m"
# # Configuration for aggregate histogram metrics
# [[aggregators.histogram]]
#   ## The period in which to flush the aggregator.
#   period = "30s"
#
#   ## If true, the original metric will be dropped by the aggregator and will not get sent to the output plugins.
#   drop_original = false
#
#   ## If true, the histogram will be reset on flush instead of accumulating the results.
#   reset = false
#
#   ## Whether bucket values should be accumulated. If set to false, "gt" tag will be added. Defaults to true.
#   cumulative = true
#
#   ## Expiration interval for each histogram. The histogram will be expired if there are no changes in any buckets for this time interval. 0 == no expiration.
#   # expiration_interval = "0m"
#
#   ## If true, aggregated histogram are pushed to output only if it was updated since previous push. Defaults to false.
#   # push_only_on_update = false
#
#   ## Example config that aggregates all fields of the metric.
#   # [[aggregators.histogram.config]]
#   #   ## Right borders of buckets (with +Inf implicitly added).
#   #   buckets = [0.0, 15.6, 34.5, 49.1, 71.5, 80.5, 94.5, 100.0]
#   #   ## The name of metric.
#   #   measurement_name = "cpu"
#
#   ## Example config that aggregates only specific fields of the metric.
#   # [[aggregators.histogram.config]]
#   #   ## Right borders of buckets (with +Inf implicitly added).
#   #   buckets = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]
#   #   ## The name of metric.
#   #   measurement_name = "diskio"
#   #   ## The concrete fields of metric
#   #   fields = ["io_time", "read_time", "write_time"]
# # Merge metrics into multifield metrics by series key
# [[aggregators.merge]]
#   ## If true, the original metric will be dropped by the aggregator and will not get sent to the output plugins.
#   drop_original = true
# # Keep the aggregate min/max of each metric passing through.
# [[aggregators.minmax]]
#   ## General Aggregator Arguments: The period on which to flush & clear the aggregator.
#   period = "30s"
#   ## If true, the original metric will be dropped by the aggregator and will not get sent to the output plugins.
#   drop_original = false
# # Keep the aggregate quantiles of each metric passing through.
# [[aggregators.quantile]]
#   ## General Aggregator Arguments: The period on which to flush & clear the aggregator.
#   period = "30s"
#
#   ## If true, the original metric will be dropped by the aggregator and will not get sent to the output plugins.
#   drop_original = false
#
#   ## Quantiles to output in the range [0,1]
#   # quantiles = [0.25, 0.5, 0.75]
#
#   ## Type of aggregation algorithm Supported are:
#   ##  "t-digest" -- approximation using centroids, can cope with large number of samples "exact R7" -- exact computation also used by Excel or NumPy (Hyndman & Fan 1996 R7) "exact R8" -- exact 
#   ## computation (Hyndman & Fan 1996 R8) NOTE: Do not use "exact" algorithms with large number of samples
#   ##       to not impair performance or memory consumption!
#   # algorithm = "t-digest"
#
#   ## Compression for approximation (t-digest). The value needs to be greater or equal to 1.0. Smaller values will result in more performance but less accuracy.
#   # compression = 100.0
# # Aggregate metrics using a Starlark script
# [[aggregators.starlark]]
#   ## The Starlark source can be set as a string in this configuration file, or by referencing a file containing the script.  Only one source or script should be set at once.
#   ##
#   ## Source of the Starlark script.
#   source = ''' state = {}
#
# def add(metric):
#   state["last"] = metric
#
# def push():
#   return state.get("last")
#
# def reset():
#   state.clear() '''
#
#   ## File containing a Starlark script.
#   # script = "/usr/local/bin/myscript.star"
#
#   ## The constants of the Starlark script.
#   # [aggregators.starlark.constants]
#   #   max_size = 10 threshold = 0.75 default_name = "Julia" debug_mode = true
# # Count the occurrence of values in fields.
# [[aggregators.valuecounter]]
#   ## General Aggregator Arguments: The period on which to flush & clear the aggregator.
#   period = "30s"
#   ## If true, the original metric will be dropped by the aggregator and will not get sent to the output plugins.
#   drop_original = false
#   ## The fields for which the values will be counted
#   fields = ["status"]
###############################################################################
#                            INPUT PLUGINS #
###############################################################################
# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states
  report_active = false
  ## If true and the info is available then add core_id and physical_id tags
  core_tags = false
# Read metrics about disk usage by mount point
[[inputs.disk]]
  ## By default stats will be gathered for all mount points. Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]
  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]
  ## Ignore mount points by mount options. The 'mount' command reports options of all mounts in parathesis. Bind mounts can be ignored with the special 'bind' option.
  # ignore_mount_opts = []
# Read metrics about disk IO by device This plugin ONLY supports Linux
[[inputs.diskio]]
  ## By default, telegraf will gather stats for all devices including disk partitions. Setting devices will restrict the stats to the specified devices. NOTE: Globbing expressions (e.g. asterix) are 
  ## not supported for
  ##       disk synonyms like '/dev/disk/by-id'.
   devices = ["vda", "sda"]
  ## Uncomment the following line if you need disk serial numbers.
   skip_serial_number = true
  #
  ## On systems which support it, device metadata can be added in the form of tags. Currently only Linux is supported via udev properties. You can view available properties for a device by running: 
  ## 'udevadm info -q property -n /dev/sda' Note: Most, but not all, udev properties can be accessed this way. Properties that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.
  # device_tags = ["ID_FS_TYPE", "ID_FS_USAGE"]
  #
  ## Using the same metadata source as device_tags, you can also customize the name of the device via templates. The 'name_templates' parameter is a list of templates to try and apply to the device. 
  ## The template may contain variables in the form of '$PROPERTY' or '${PROPERTY}'. The first template which does not contain any variables not present for the device is used as the device name 
  ## tag. The typical use case is for LVM volumes, to get the VG/LV name instead of the near-meaningless DM-0 name.
  # name_templates = ["$ID_FS_LABEL","$DM_VG_NAME/$DM_LV_NAME"]
# Get kernel statistics from /proc/stat This plugin ONLY supports Linux
[[inputs.kernel]]
  # no configuration
# Read metrics about memory usage
[[inputs.mem]]
  # no configuration
# Get the number of processes and group them by status This plugin ONLY supports non-Windows
[[inputs.processes]]
  ## Use sudo to run ps command on *BSD systems. Linux systems will read /proc, so this does not apply there.
  use_sudo = false
# Read metrics about swap memory usage
[[inputs.swap]]
  # no configuration
# Read metrics about system load & uptime
[[inputs.system]]
  # no configuration
# # Gather ActiveMQ metrics
# [[inputs.activemq]]
#   ## ActiveMQ WebConsole URL
#   url = "http://127.0.0.1:8161"
#
#   ## Required ActiveMQ Endpoint
#   ##   deprecated in 1.11; use the url option
#   # server = "192.168.50.10" port = 8161
#
#   ## Credentials for basic HTTP authentication
#   # username = "admin" password = "admin"
#
#   ## Required ActiveMQ webadmin root path
#   # webadmin = "admin"
#
#   ## Maximum time to receive response.
#   # response_timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Read stats from aerospike server(s)
# [[inputs.aerospike]]
#   ## Aerospike servers to connect to (with port) This plugin will query all namespaces the aerospike server has configured and get stats for them.
#   servers = ["localhost:3000"]
#
#   # username = "telegraf" password = "pa$$word"
#
#   ## Optional TLS Config
#   # enable_tls = false tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem" tls_name = "tlsname"
#   ## If false, skip chain & host verification
#   # insecure_skip_verify = true
#
#   # Feature Options Add namespace variable to limit the namespaces executed on Leave blank to do all disable_query_namespaces = true # default false namespaces = ["namespace1", "namespace2"]
#
#   # Enable set level telemetry query_sets = true # default: false Add namespace set combinations to limit sets executed on Leave blank to do all sets sets = ["namespace1/set1", "namespace1/set2", 
#   # "namespace3"]
#
#   # Histograms enable_ttl_histogram = true # default: false enable_object_size_linear_histogram = true # default: false
#
#   # by default, aerospike produces a 100 bucket histogram this is not great for most graphing tools, this will allow the ability to squash this to a smaller number of buckets To have a balanced 
#   # histogram, the number of buckets chosen should divide evenly into 100. num_histogram_buckets = 100 # default: 10
# # Query statistics from AMD Graphics cards using rocm-smi binary
# [[inputs.amd_rocm_smi]]
#   ## Optional: path to rocm-smi binary, defaults to $PATH via exec.LookPath
#   # bin_path = "/opt/rocm/bin/rocm-smi"
#
#   ## Optional: timeout for GPU polling
#   # timeout = "5s"
# # Read Apache status information (mod_status)
# [[inputs.apache]]
#   ## An array of URLs to gather from, must be directed at the machine readable version of the mod_status page including the auto query string. Default is "http://localhost/server-status?auto".
#   urls = ["http://localhost/server-status?auto"]
#
#   ## Credentials for basic HTTP authentication.
#   # username = "myuser" password = "mypassword"
#
#   ## Maximum time to receive response.
#   # response_timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Monitor APC UPSes connected to apcupsd
# [[inputs.apcupsd]]
#   # A list of running apcupsd server to connect to. If not provided will default to tcp://127.0.0.1:3551
#   servers = ["tcp://127.0.0.1:3551"]
#
#   ## Timeout for dialing server.
#   timeout = "5s"
# # Gather metrics from Apache Aurora schedulers
# [[inputs.aurora]]
#   ## Schedulers are the base addresses of your Aurora Schedulers
#   schedulers = ["http://127.0.0.1:8081"]
#
#   ## Set of role types to collect metrics from.
#   ##
#   ## The scheduler roles are checked each interval by contacting the scheduler nodes; zookeeper is not contacted.
#   # roles = ["leader", "follower"]
#
#   ## Timeout is the max time for total network operations.
#   # timeout = "5s"
#
#   ## Username and password are sent using HTTP Basic Auth.
#   # username = "username" password = "pa$$word"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Gather Azure resources metrics from Azure Monitor API
# [[inputs.azure_monitor]]
#   # can be found under Overview->Essentials in the Azure portal for your application/service
#   subscription_id = "<<SUBSCRIPTION_ID>>"
#   # can be obtained by registering an application under Azure Active Directory
#   client_id = "<<CLIENT_ID>>"
#   # can be obtained by registering an application under Azure Active Directory
#   client_secret = "<<CLIENT_SECRET>>"
#   # can be found under Azure Active Directory->Properties
#   tenant_id = "<<TENANT_ID>>"
#
#   # resource target #1 to collect metrics from
#   [[inputs.azure_monitor.resource_target]]
#     # can be found undet Overview->Essentials->JSON View in the Azure portal for your application/service must start with 'resourceGroups/...' ('/subscriptions/xxxxxxxx-xxxx-xxxx-xxx-xxxxxxxxxxxx' 
#     # must be removed from the beginning of Resource ID property value)
#     resource_id = "<<RESOURCE_ID>>"
#     # the metric names to collect leave the array empty to use all metrics available to this resource
#     metrics = [ "<<METRIC>>", "<<METRIC>>" ]
#     # metrics aggregation type value to collect can be 'Total', 'Count', 'Average', 'Minimum', 'Maximum' leave the array empty to collect all aggregation types values for each metric
#     aggregations = [ "<<AGGREGATION>>", "<<AGGREGATION>>" ]
#
#   # resource target #2 to collect metrics from
#   [[inputs.azure_monitor.resource_target]]
#     resource_id = "<<RESOURCE_ID>>" metrics = [ "<<METRIC>>", "<<METRIC>>" ] aggregations = [ "<<AGGREGATION>>", "<<AGGREGATION>>" ]
#
#   # resource group target #1 to collect metrics from resources under it with resource type
#   [[inputs.azure_monitor.resource_group_target]]
#     # the resource group name
#     resource_group = "<<RESOURCE_GROUP_NAME>>"
#
#     # defines the resources to collect metrics from
#     [[inputs.azure_monitor.resource_group_target.resource]]
#       # the resource type
#       resource_type = "<<RESOURCE_TYPE>>" metrics = [ "<<METRIC>>", "<<METRIC>>" ] aggregations = [ "<<AGGREGATION>>", "<<AGGREGATION>>" ]
#
#     # defines the resources to collect metrics from
#     [[inputs.azure_monitor.resource_group_target.resource]]
#       resource_type = "<<RESOURCE_TYPE>>" metrics = [ "<<METRIC>>", "<<METRIC>>" ] aggregations = [ "<<AGGREGATION>>", "<<AGGREGATION>>" ]
#
#   # resource group target #2 to collect metrics from resources under it with resource type
#   [[inputs.azure_monitor.resource_group_target]]
#     resource_group = "<<RESOURCE_GROUP_NAME>>"
#
#     [[inputs.azure_monitor.resource_group_target.resource]]
#       resource_type = "<<RESOURCE_TYPE>>" metrics = [ "<<METRIC>>", "<<METRIC>>" ] aggregations = [ "<<AGGREGATION>>", "<<AGGREGATION>>" ]
#
#   # subscription target #1 to collect metrics from resources under it with resource type
#   [[inputs.azure_monitor.subscription_target]]
#     resource_type = "<<RESOURCE_TYPE>>" metrics = [ "<<METRIC>>", "<<METRIC>>" ] aggregations = [ "<<AGGREGATION>>", "<<AGGREGATION>>" ]
#
#   # subscription target #2 to collect metrics from resources under it with resource type
#   [[inputs.azure_monitor.subscription_target]]
#     resource_type = "<<RESOURCE_TYPE>>" metrics = [ "<<METRIC>>", "<<METRIC>>" ] aggregations = [ "<<AGGREGATION>>", "<<AGGREGATION>>" ]
# # Gather Azure Storage Queue metrics
# [[inputs.azure_storage_queue]]
#   ## Required Azure Storage Account name
#   account_name = "mystorageaccount"
#
#   ## Required Azure Storage Account access key
#   account_key = "storageaccountaccesskey"
#
#   ## Set to false to disable peeking age of oldest message (executes faster)
#   # peek_oldest_message_age = true
# # Read metrics of bcache from stats_total and dirty_data This plugin ONLY supports Linux
# [[inputs.bcache]]
#   ## Bcache sets path If not specified, then default is:
#   bcachePath = "/sys/fs/bcache"
#
#   ## By default, Telegraf gather stats for all bcache devices Setting devices will restrict the stats to the specified bcache devices.
#   bcacheDevs = ["bcache0"]
# # Collects Beanstalkd server and tubes stats
# [[inputs.beanstalkd]]
#   ## Server to collect data from
#   server = "localhost:11300"
#
#   ## List of tubes to gather stats about. If no tubes specified then data gathered for each tube on server reported by list-tubes command
#   tubes = ["notifications"]
# # Read metrics exposed by Beat
# [[inputs.beat]]
#   ## An URL from which to read Beat-formatted JSON Default is "http://127.0.0.1:5066".
#   url = "http://127.0.0.1:5066"
#
#   ## Enable collection of the listed stats An empty list means collect all. Available options are currently "beat", "libbeat", "system" and "filebeat".
#   # include = ["beat", "libbeat", "filebeat"]
#
#   ## HTTP method
#   # method = "GET"
#
#   ## Optional HTTP headers
#   # headers = {"X-Special-Header" = "Special-Value"}
#
#   ## Override HTTP "Host" header
#   # host_header = "logstash.example.com"
#
#   ## Timeout for HTTP requests
#   # timeout = "5s"
#
#   ## Optional HTTP Basic Auth credentials
#   # username = "username" password = "pa$$word"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Read BIND nameserver XML statistics
# [[inputs.bind]]
#   ## An array of BIND XML statistics URI to gather stats. Default is "http://localhost:8053/xml/v3".
#   # urls = ["http://localhost:8053/xml/v3"] gather_memory_contexts = false gather_views = false
#
#   ## Timeout for http requests made by bind nameserver
#   # timeout = "4s"
# # Collect bond interface status, slaves statuses and failures count
# [[inputs.bond]]
#   ## Sets 'proc' directory path If not specified, then default is /proc
#   # host_proc = "/proc"
#
#   ## Sets 'sys' directory path If not specified, then default is /sys
#   # host_sys = "/sys"
#
#   ## By default, telegraf gather stats for all bond interfaces Setting interfaces will restrict the stats to the specified bond interfaces.
#   # bond_interfaces = ["bond0"]
#
#   ## Tries to collect additional bond details from /sys/class/net/{bond} currently only useful for LACP (mode 4) bonds
#   # collect_sys_details = false
# # Collect Kafka topics and consumers status from Burrow HTTP API.
# [[inputs.burrow]]
#   ## Burrow API endpoints in format "schema://host:port". Default is "http://localhost:8000".
#   servers = ["http://localhost:8000"]
#
#   ## Override Burrow API prefix. Useful when Burrow is behind reverse-proxy.
#   # api_prefix = "/v3/kafka"
#
#   ## Maximum time to receive response.
#   # response_timeout = "5s"
#
#   ## Limit per-server concurrent connections. Useful in case of large number of topics or consumer groups.
#   # concurrent_connections = 20
#
#   ## Filter clusters, default is no filtering. Values can be specified as glob patterns.
#   # clusters_include = [] clusters_exclude = []
#
#   ## Filter consumer groups, default is no filtering. Values can be specified as glob patterns.
#   # groups_include = [] groups_exclude = []
#
#   ## Filter topics, default is no filtering. Values can be specified as glob patterns.
#   # topics_include = [] topics_exclude = []
#
#   ## Credentials for basic HTTP authentication.
#   # username = "" password = ""
#
#   ## Optional SSL config
#   # ssl_ca = "/etc/telegraf/ca.pem" ssl_cert = "/etc/telegraf/cert.pem" ssl_key = "/etc/telegraf/key.pem" insecure_skip_verify = false
# # Collects performance metrics from the MON, OSD, MDS and RGW nodes in a Ceph storage cluster.
# [[inputs.ceph]]
#   ## This is the recommended interval to poll. Too frequent and you will lose data points due to timeouts during rebalancing and recovery
#   interval = '1m'
#
#   ## All configuration values are optional, defaults are shown below
#
#   ## location of ceph binary
#   ceph_binary = "/usr/bin/ceph"
#
#   ## directory in which to look for socket files
#   socket_dir = "/var/run/ceph"
#
#   ## prefix of MON and OSD socket files, used to determine socket type
#   mon_prefix = "ceph-mon" osd_prefix = "ceph-osd" mds_prefix = "ceph-mds" rgw_prefix = "ceph-client"
#
#   ## suffix used to identify socket files
#   socket_suffix = "asok"
#
#   ## Ceph user to authenticate as, ceph will search for the corresponding keyring e.g. client.admin.keyring in /etc/ceph, or the explicit path defined in the client section of ceph.conf for 
#   ## example:
#   ##
#   ##     [client.telegraf]
#   ##         keyring = /etc/ceph/client.telegraf.keyring
#   ##
#   ## Consult the ceph documentation for more detail on keyring generation.
#   ceph_user = "client.admin"
#
#   ## Ceph configuration to use to locate the cluster
#   ceph_config = "/etc/ceph/ceph.conf"
#
#   ## Whether to gather statistics via the admin socket
#   gather_admin_socket_stats = true
#
#   ## Whether to gather statistics via ceph commands, requires ceph_user and ceph_config to be specified
#   gather_cluster_stats = false
# # Read specific statistics per cgroup This plugin ONLY supports Linux
# [[inputs.cgroup]]
#   ## Directories in which to look for files, globs are supported. Consider restricting paths to the set of cgroups you really want to monitor if you have a large number of cgroups, to avoid any 
#   ## cardinality issues.
#   # paths = [
#   #   "/sys/fs/cgroup/memory", "/sys/fs/cgroup/memory/child1", "/sys/fs/cgroup/memory/child2/*", ]
#   ## cgroup stat fields, as file names, globs are supported. these file names are appended to each path from above.
#   # files = ["memory.*usage*", "memory.limit_in_bytes"]
# # Get standard chrony metrics, requires chronyc executable.
# [[inputs.chrony]]
#   ## If true, chronyc tries to perform a DNS lookup for the time server.
#   # dns_lookup = false
# # Pull Metric Statistics from Amazon CloudWatch
# [[inputs.cloudwatch]]
#   ## Amazon Region
#   region = "us-east-1"
#
#   ## Amazon Credentials Credentials are loaded in the following order 1) Web identity provider credentials via STS if role_arn and
#   ##    web_identity_token_file are specified 2) Assumed credentials via STS if role_arn is specified 3) explicit credentials from 'access_key' and 'secret_key' 4) shared profile from 'profile' 5) 
#   ## environment variables 6) shared credentials file 7) EC2 Instance Profile
#   # access_key = "" secret_key = "" token = "" role_arn = "" web_identity_token_file = "" role_session_name = "" profile = "" shared_credential_file = ""
#
#   ## Endpoint to make request against, the correct endpoint is automatically determined and this option should only be set if you wish to override the default.
#   ##   ex: endpoint_url = "http://localhost:8000"
#   # endpoint_url = ""
#
#   ## Set http_proxy
#   # use_system_proxy = false http_proxy_url = "http://localhost:8888"
#
#   ## The minimum period for Cloudwatch metrics is 1 minute (60s). However not all metrics are made available to the 1 minute period. Some are collected at 3 minute, 5 minute, or larger intervals. 
#   ## See https://aws.amazon.com/cloudwatch/faqs/#monitoring. Note that if a period is configured that is smaller than the minimum for a particular metric, that metric will not be returned by the 
#   ## Cloudwatch API and will not be collected by Telegraf.
#   #
#   ## Requested CloudWatch aggregation Period (required) Must be a multiple of 60s.
#   period = "5m"
#
#   ## Collection Delay (required) Must account for metrics availability via CloudWatch API
#   delay = "5m"
#
#   ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid gaps or overlap in pulled data
#   interval = "5m"
#
#   ## Recommended if "delay" and "period" are both within 3 hours of request time. Invalid values will be ignored. Recently Active feature will only poll for CloudWatch ListMetrics values that 
#   ## occurred within the last 3h. If enabled, it will reduce total API usage of the CloudWatch ListMetrics API and require less memory to retain. Do not enable if "period" or "delay" is longer 
#   ## than 3 hours, as it will not return data more than 3 hours old. See https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_ListMetrics.html
#   #recently_active = "PT3H"
#
#   ## Configure the TTL for the internal cache of metrics.
#   # cache_ttl = "1h"
#
#   ## Metric Statistic Namespaces (required)
#   namespaces = ["AWS/ELB"]
#
#   ## Maximum requests per second. Note that the global default AWS rate limit is 50 reqs/sec, so if you define multiple namespaces, these should add up to a maximum of 50. See 
#   ## http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_limits.html
#   # ratelimit = 25
#
#   ## Timeout for http requests made by the cloudwatch client.
#   # timeout = "5s"
#
#   ## Batch Size The size of each batch to send requests to Cloudwatch. 500 is the suggested largest size. If a request gets to large (413 errors), consider reducing this amount.
#   # batch_size = 500
#
#   ## Namespace-wide statistic filters. These allow fewer queries to be made to cloudwatch.
#   # statistic_include = ["average", "sum", "minimum", "maximum", sample_count"] statistic_exclude = []
#
#   ## Metrics to Pull Defaults to all Metrics in Namespace if nothing is provided Refreshes Namespace available metrics every 1h
#   #[[inputs.cloudwatch.metrics]]
#   #  names = ["Latency", "RequestCount"]
#   #
#   #  ## Statistic filters for Metric.  These allow for retrieving specific statistics for an individual metric.
#   #  # statistic_include = ["average", "sum", "minimum", "maximum", sample_count"] statistic_exclude = []
#   #
#   #  ## Dimension filters for Metric. All dimensions defined for the metric names must be specified in order to retrieve the metric statistics. 'value' has wildcard / 'glob' matching support such 
#   #  ## as 'p-*'.
#   #  [[inputs.cloudwatch.metrics.dimensions]]
#   #    name = "LoadBalancerName" value = "p-example"
# # Collects conntrack stats from the configured directories and files. This plugin ONLY supports Linux
# [[inputs.conntrack]]
#   ## The following defaults would work with multiple versions of conntrack. Note the nf_ and ip_ filename prefixes are mutually exclusive across kernel versions, as are the directory locations.
#
#   ## Superset of filenames to look for within the conntrack dirs. Missing files will be ignored.
#   files = ["ip_conntrack_count","ip_conntrack_max",
#           "nf_conntrack_count","nf_conntrack_max"]
#
#   ## Directories to search within for the conntrack files above. Missing directories will be ignored.
#   dirs = ["/proc/sys/net/ipv4/netfilter","/proc/sys/net/netfilter"]
#   ## all - aggregated statistics percpu - include detailed statistics with cpu tag
#   collect = ["all", "percpu"]
# # Gather health check statuses from services registered in Consul
# [[inputs.consul]]
#   ## Consul server address
#   # address = "localhost:8500"
#
#   ## URI scheme for the Consul server, one of "http", "https"
#   # scheme = "http"
#
#   ## Metric version controls the mapping from Consul metrics into Telegraf metrics. Version 2 moved all fields with string values to tags.
#   ##
#   ##   example: metric_version = 1; deprecated in 1.16
#   ##            metric_version = 2; recommended version
#   # metric_version = 1
#
#   ## ACL token used in every request
#   # token = ""
#
#   ## HTTP Basic Authentication username and password.
#   # username = "" password = ""
#
#   ## Data center to query the health checks from
#   # datacenter = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = true
#
#   ## Consul checks' tag splitting
#   # When tags are formatted like "key:value" with ":" as a delimiter then they will be splitted and reported as proper key:value in Telegraf tag_delimiter = ":"
# # Read metrics from the Consul Agent API
# [[inputs.consul_agent]]
#   ## URL for the Consul agent
#   # url = "http://127.0.0.1:8500"
#
#   ## Use auth token for authorization. If both are set, an error is thrown. If both are empty, no token will be used.
#   # token_file = "/path/to/auth/token"
#   ## OR
#   # token = "a1234567-40c7-9048-7bae-378687048181"
#
#   ## Set timeout (default 5 seconds)
#   # timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = /path/to/cafile tls_cert = /path/to/certfile tls_key = /path/to/keyfile
# # Read per-node and per-bucket metrics from Couchbase
# [[inputs.couchbase]]
#   ## specify servers via a url matching:
#   ##  [protocol://][:password]@address[:port] e.g.
#   ##    http://couchbase-0.example.com/ http://admin:secret@couchbase-0.example.com:8091/
#   ##
#   ## If no servers are specified, then localhost is used as the host. If no protocol is specified, HTTP is used. If no port is specified, 8091 is used.
#   servers = ["http://localhost:8091"]
#
#   ## Filter bucket fields to include only here.
#   # bucket_stats_included = ["quota_percent_used", "ops_per_sec", "disk_fetches", "item_count", "disk_used", "data_used", "mem_used"]
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification (defaults to false) If set to false, tls_cert and tls_key are required
#   # insecure_skip_verify = false
#
#   ## Whether to collect cluster-wide bucket statistics It is recommended to disable this in favor of node_stats to get a better view of the cluster.
#   cluster_bucket_stats = true
#
#   ## Whether to collect bucket stats for each individual node
#   node_bucket_stats = false
# # Read CouchDB Stats from one or more servers
# [[inputs.couchdb]]
#   ## Works with CouchDB stats endpoints out of the box Multiple Hosts from which to read CouchDB stats:
#   hosts = ["http://localhost:8086/_stats"]
#
#   ## Use HTTP Basic Authentication.
#   # basic_username = "telegraf" basic_password = "p@ssw0rd"
# # Fetch metrics from a CSGO SRCDS
# [[inputs.csgo]]
#   ## Specify servers using the following format:
#   ##    servers = [
#   ##      ["ip1:port1", "rcon_password1"], ["ip2:port2", "rcon_password2"], ]
#   #
#   ## If no servers are specified, no data will be collected
#   servers = []
# # Input plugin for DC/OS metrics
# [[inputs.dcos]]
#   ## The DC/OS cluster URL.
#   cluster_url = "https://dcos-master-1"
#
#   ## The ID of the service account.
#   service_account_id = "telegraf"
#   ## The private key file for the service account.
#   service_account_private_key = "/etc/telegraf/telegraf-sa-key.pem"
#
#   ## Path containing login token.  If set, will read on every gather.
#   # token_file = "/home/dcos/.dcos/token"
#
#   ## In all filter options if both include and exclude are empty all items will be collected.  Arrays may contain glob patterns.
#   ##
#   ## Node IDs to collect metrics from.  If a node is excluded, no metrics will be collected for its containers or apps.
#   # node_include = [] node_exclude = []
#   ## Container IDs to collect container metrics from.
#   # container_include = [] container_exclude = []
#   ## Container IDs to collect app metrics from.
#   # app_include = [] app_exclude = []
#
#   ## Maximum concurrent connections to the cluster.
#   # max_connections = 10
#   ## Maximum time to receive a response from cluster.
#   # response_timeout = "20s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## If false, skip chain & host verification
#   # insecure_skip_verify = true
#
#   ## Recommended filtering to reduce series cardinality.
#   # [inputs.dcos.tagdrop]
#   #   path = ["/var/lib/mesos/slave/slaves/*"]
# # Read metrics from one or many disque servers
# [[inputs.disque]]
#   ## An array of URI to gather stats about. Specify an ip or hostname with optional port and password. ie disque://localhost, disque://10.10.3.33:18832, 10.0.0.1:10000, etc. If no servers are 
#   ## specified, then localhost is used as the host.
#   servers = ["localhost"]
# # Provide a native collection for dmsetup based statistics for dm-cache This plugin ONLY supports Linux
# [[inputs.dmcache]]
#   ## Whether to report per-device stats or not
#   per_device = true
# # Query given DNS server and gives statistics
# [[inputs.dns_query]]
#   ## servers to query
#   servers = ["8.8.8.8"]
#
#   ## Network is the network protocol name.
#   # network = "udp"
#
#   ## Domains or subdomains to query.
#   # domains = ["."]
#
#   ## Query record type. Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.
#   # record_type = "A"
#
#   ## Dns server port.
#   # port = 53
#
#   ## Query timeout
#   # timeout = "2s"
#
#   ## Include the specified additional properties in the resulting metric. The following values are supported:
#   ##    "first_ip" -- return IP of the first A and AAAA answer "all_ips" -- return IPs of all A and AAAA answers
#   # include_fields = []
# # Read metrics about docker containers
 [[inputs.docker]]
#   ## Docker Endpoint
#   ##   To use TCP, set endpoint = "tcp://[ip]:[port]" To use environment variables (ie, docker-machine), set endpoint = "ENV"
   endpoint = "unix:///var/run/docker.sock"
#
#   ## Set to true to collect Swarm metrics(desired_replicas, running_replicas) Note: configure this in one of the manager nodes in a Swarm cluster. configuring in multiple Swarm managers results in 
#   ## duplication of metrics.
#   gather_services = false
#
#   ## Only collect metrics for these containers. Values will be appended to container_name_include. Deprecated (1.4.0), use container_name_include
#   container_names = []
#
#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars
   source_tag = false
#
#   ## Containers to include and exclude. Collect all if empty. Globs accepted.
   container_name_include = [] 
   container_name_exclude = []
#
#   ## Container states to include and exclude. Globs accepted. When empty only containers in the "running" state will be captured. example: container_state_include = ["created", "restarting", 
#   ## "running", "removing", "paused", "exited", "dead"] example: container_state_exclude = ["created", "restarting", "running", "removing", "paused", "exited", "dead"]
#   # container_state_include = [] container_state_exclude = []
#
#   ## Timeout for docker list, info, and stats commands
   timeout = "5s"
#
#   ## Whether to report for each container per-device blkio (8:0, 8:1...), network (eth0, eth1, ...) and cpu (cpu0, cpu1, ...) stats or not. Usage of this setting is discouraged since it will be 
#   ## deprecated in favor of 'perdevice_include'. Default value is 'true' for backwards compatibility, please set it to 'false' so that 'perdevice_include' setting is honored.
#   perdevice = true
#
#   ## Specifies for which classes a per-device metric should be issued Possible values are 'cpu' (cpu0, cpu1, ...), 'blkio' (8:0, 8:1, ...) and 'network' (eth0, eth1, ...) Please note that this 
#   ## setting has no effect if 'perdevice' is set to 'true'
#   # perdevice_include = ["cpu"]
#
#   ## Whether to report for each container total blkio and network stats or not. Usage of this setting is discouraged since it will be deprecated in favor of 'total_include'. Default value is 
#   ## 'false' for backwards compatibility, please set it to 'true' so that 'total_include' setting is honored.
#   total = false
#
#   ## Specifies for which classes a total metric should be issued. Total is an aggregated of the 'perdevice' values. Possible values are 'cpu', 'blkio' and 'network' Total 'cpu' is reported 
#   ## directly by Docker daemon, and 'network' and 'blkio' totals are aggregated by this plugin. Please note that this setting has no effect if 'total' is set to 'false'
#   # total_include = ["cpu", "blkio", "network"]
#
#   ## docker labels to include and exclude as tags.  Globs accepted. Note that an empty array for both will include all labels as tags
    docker_label_include = [] 
    docker_label_exclude = []
#
#   ## Which environment variables should we use as a tag
#   tag_env = ["JAVA_HOME", "HEAP_SIZE"]
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Read metrics about dovecot servers
# [[inputs.dovecot]]
#   ## specify dovecot servers via an address:port list
#   ##  e.g.
#   ##    localhost:24242 or as an UDS socket e.g. /var/run/dovecot/old-stats
#   ##
#   ## If no servers are specified, then localhost is used as the host.
#   servers = ["localhost:24242"]
#
#   ## Type is one of "user", "domain", "ip", or "global"
#   type = "global"
#
#   ## Wildcard matches like "*.com". An empty string "" is same as "*" If type = "ip" filters should be <IP/network>
#   filters = [""]
# # Reads metrics from DPDK applications using v2 telemetry interface. This plugin ONLY supports Linux
# [[inputs.dpdk]]
#   ## Path to DPDK telemetry socket. This shall point to v2 version of DPDK telemetry interface.
#   # socket_path = "/var/run/dpdk/rte/dpdk_telemetry.v2"
#
#   ## Duration that defines how long the connected socket client will wait for a response before terminating connection. This includes both writing to and reading from socket. Since it's local 
#   ## socket access to a fast packet processing application, the timeout should be sufficient for most users. Setting the value to 0 disables the timeout (not recommended)
#   # socket_access_timeout = "200ms"
#
#   ## Enables telemetry data collection for selected device types. Adding "ethdev" enables collection of telemetry from DPDK NICs (stats, xstats, link_status). Adding "rawdev" enables collection of 
#   ## telemetry from DPDK Raw Devices (xstats).
#   # device_types = ["ethdev"]
#
#   ## List of custom, application-specific telemetry commands to query The list of available commands depend on the application deployed. Applications can register their own commands via telemetry 
#   ## library API http://doc.dpdk.org/guides/prog_guide/telemetry_lib.html#registering-commands For L3 Forwarding with Power Management Sample Application this could be:
#   ##   additional_commands = ["/l3fwd-power/stats"]
#   # additional_commands = []
#
#   ## Allows turning off collecting data for individual "ethdev" commands. Remove "/ethdev/link_status" from list to gather link status metrics.
#   [inputs.dpdk.ethdev]
#     exclude_commands = ["/ethdev/link_status"]
#
#   ## When running multiple instances of the plugin it's recommended to add a unique tag to each instance to identify metrics exposed by an instance of DPDK application. This is useful when 
#   ## multiple DPDK apps run on a single host.
#   ##  [inputs.dpdk.tags]
#   ##    dpdk_instance = "my-fwd-app"
# # Read metrics about ECS containers
# [[inputs.ecs]]
#   ## ECS metadata url. Metadata v2 API is used if set explicitly. Otherwise, v3 metadata endpoint API is used if available.
#   # endpoint_url = ""
#
#   ## Containers to include and exclude. Globs accepted. Note that an empty array for both will include all containers
#   # container_name_include = [] container_name_exclude = []
#
#   ## Container states to include and exclude. Globs accepted. When empty only containers in the "RUNNING" state will be captured. Possible values are "NONE", "PULLED", "CREATED", "RUNNING", 
#   ## "RESOURCES_PROVISIONED", "STOPPED".
#   # container_status_include = [] container_status_exclude = []
#
#   ## ecs labels to include and exclude as tags.  Globs accepted. Note that an empty array for both will include all labels as tags
#   ecs_label_include = [ "com.amazonaws.ecs.*" ] ecs_label_exclude = []
#
#   ## Timeout for queries.
#   # timeout = "5s"
# # Read stats from one or more Elasticsearch servers or clusters
# [[inputs.elasticsearch]]
#   ## specify a list of one or more Elasticsearch servers you can add username and password to your url to use basic authentication: servers = ["http://user:pass@localhost:9200"]
#   servers = ["http://localhost:9200"]
#
#   ## Timeout for HTTP requests to the elastic search server(s)
#   http_timeout = "5s"
#
#   ## When local is true (the default), the node will read only its own stats. Set local to false when you want to read the node stats from all nodes of the cluster.
#   local = true
#
#   ## Set cluster_health to true when you want to obtain cluster health stats
#   cluster_health = false
#
#   ## Adjust cluster_health_level when you want to obtain detailed health stats The options are
#   ##  - indices (default) - cluster
#   # cluster_health_level = "indices"
#
#   ## Set cluster_stats to true when you want to obtain cluster stats.
#   cluster_stats = false
#
#   ## Only gather cluster_stats from the master node. To work this require local = true
#   cluster_stats_only_from_master = true
#
#   ## Indices to collect; can be one or more indices names or _all Use of wildcards is allowed. Use a wildcard at the end to retrieve index names that end with a changing value, like a date.
#   indices_include = ["_all"]
#
#   ## One of "shards", "cluster", "indices" Currently only "shards" is implemented
#   indices_level = "shards"
#
#   ## node_stats is a list of sub-stats that you want to have gathered. Valid options are "indices", "os", "process", "jvm", "thread_pool", "fs", "transport", "http", "breaker". Per default, all 
#   ## stats are gathered.
#   # node_stats = ["jvm", "http"]
#
#   ## HTTP Basic Authentication username and password.
#   # username = "" password = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Sets the number of most recent indices to return for indices that are configured with a date-stamped suffix. Each 'indices_include' entry ending with a wildcard (*) or glob matching pattern 
#   ## will group together all indices that match it, and sort them by the date or number after the wildcard. Metrics then are gathered for only the 'num_most_recent_indices' amount of most recent 
#   ## indices.
#   # num_most_recent_indices = 0
# # Derive metrics from aggregating Elasticsearch query results
# [[inputs.elasticsearch_query]]
#   ## The full HTTP endpoint URL for your Elasticsearch instance Multiple urls can be specified as part of the same cluster, this means that only ONE of the urls will be written to each interval.
#   urls = [ "http://node1.es.example.com:9200" ] # required.
#
#   ## Elasticsearch client timeout, defaults to "5s".
#   # timeout = "5s"
#
#   ## Set to true to ask Elasticsearch a list of all cluster nodes, thus it is not necessary to list all nodes in the urls config option
#   # enable_sniffer = false
#
#   ## Set the interval to check if the Elasticsearch nodes are available This option is only used if enable_sniffer is also set (0s to disable it)
#   # health_check_interval = "10s"
#
#   ## HTTP basic authentication details (eg. when using x-pack)
#   # username = "telegraf" password = "mypassword"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   [[inputs.elasticsearch_query.aggregation]]
#     ## measurement name for the results of the aggregation query
#     measurement_name = "measurement"
#
#     ## Elasticsearch indexes to query (accept wildcards).
#     index = "index-*"
#
#     ## The date/time field in the Elasticsearch index (mandatory).
#     date_field = "@timestamp"
#
#     ## If the field used for the date/time field in Elasticsearch is also using a custom date/time format it may be required to provide the format to correctly parse the field.
#     ##
#     ## If using one of the built in elasticsearch formats this is not required.
#     # date_field_custom_format = ""
#
#     ## Time window to query (eg. "1m" to query documents from last minute). Normally should be set to same as collection interval
#     query_period = "1m"
#
#     ## Lucene query to filter results
#     # filter_query = "*"
#
#     ## Fields to aggregate values (must be numeric fields)
#     # metric_fields = ["metric"]
#
#     ## Aggregation function to use on the metric fields Must be set if 'metric_fields' is set Valid values are: avg, sum, min, max, sum
#     # metric_function = "avg"
#
#     ## Fields to be used as tags Must be text, non-analyzed fields. Metric aggregations are performed per tag
#     # tags = ["field.keyword", "field2.keyword"]
#
#     ## Set to true to not ignore documents when the tag(s) above are missing
#     # include_missing_tag = false
#
#     ## String value of the tag when the tag does not exist Used when include_missing_tag is true
#     # missing_tag_value = "null"
# # Returns ethtool statistics for given interfaces This plugin ONLY supports Linux
# [[inputs.ethtool]]
#   ## List of interfaces to pull metrics for
#   # interface_include = ["eth0"]
#
#   ## List of interfaces to ignore when pulling metrics.
#   # interface_exclude = ["eth1"]
#
#   ## Plugin behavior for downed interfaces Available choices:
#   ##   - expose: collect & report metrics for down interfaces - skip: ignore interfaces that are marked down
#   # down_interfaces = "expose"
#
#   ## Reading statistics from interfaces in additional namespaces is also supported, so long as the namespaces are named (have a symlink in /var/run/netns). The telegraf process will also need the 
#   ## CAP_SYS_ADMIN permission. By default, only the current namespace will be used. For additional namespace support, at least one of `namespace_include` and `namespace_exclude` must be provided. 
#   ## To include all namespaces, set `namespace_include` to `["*"]`. The initial namespace (if anonymous) can be specified with the empty string ("").
#
#   ## List of namespaces to pull metrics for
#   # namespace_include = []
#
#   ## List of namespace to ignore when pulling metrics.
#   # namespace_exclude = []
#
#   ## Some drivers declare statistics with extra whitespace, different spacing, and mix cases. This list, when enabled, can be used to clean the keys. Here are the current possible normalizations:
#   ##  * snakecase: converts fooBarBaz to foo_bar_baz * trim: removes leading and trailing whitespace * lower: changes all capitalized letters to lowercase * underscore: replaces spaces with 
#   ##  underscores
#   # normalize_keys = ["snakecase", "trim", "lower", "underscore"]
# # Read metrics from one or more commands that can output to stdout
# [[inputs.exec]]
#   ## Commands array
#   commands = [
#     "/tmp/test.sh", "/usr/bin/mycollector --foo=bar", "/tmp/collect_*.sh" ]
#
#   ## Environment variables Array of "key=value" pairs to pass as environment variables e.g. "KEY=value", "USERNAME=John Doe", "LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs"
#   # environment = []
#
#   ## Timeout for each command to complete.
#   timeout = "5s"
#
#   ## measurement name suffix (for separating different commands)
#   name_suffix = "_mycollector"
#
#   ## Data format to consume. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"
# # Read metrics from fail2ban.
# [[inputs.fail2ban]]
#   ## Use sudo to run fail2ban-client
#   use_sudo = false
# # Read devices value(s) from a Fibaro controller
# [[inputs.fibaro]]
#   ## Required Fibaro controller address/hostname. Note: at the time of writing this plugin, Fibaro only implemented http - no https available
#   url = "http://<controller>:80"
#
#   ## Required credentials to access the API (http://<controller/api/<component>)
#   username = "<username>" password = "<password>"
#
#   ## Amount of time allowed to complete the HTTP request
#   # timeout = "5s"
# # Parse a complete file each interval
# [[inputs.file]]
#   ## Files to parse each interval.  Accept standard unix glob matching rules, as well as ** to match recursive files and directories.
#   files = ["/tmp/metrics.out"]
#
#   ## Character encoding to use when interpreting the file contents.  Invalid characters are replaced using the unicode replacement character.  When set to the empty string the data is not decoded 
#   ## to text.
#   ##   ex: character_encoding = "utf-8"
#   ##       character_encoding = "utf-16le" character_encoding = "utf-16be" character_encoding = ""
#   # character_encoding = ""
#
#   ## Data format to consume. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"
#
#
#   ## Name a tag containing the name of the file the data was parsed from.  Leave empty to disable. Cautious when file name variation is high, this can increase the cardinality significantly. Read 
#   ## more about cardinality here: https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality
#   # file_tag = ""
# # Count files in a directory
# [[inputs.filecount]]
#   ## Directories to gather stats about. This accept standard unit glob matching rules, but with the addition of ** as a "super asterisk". ie:
#   ##   /var/log/** -> recursively find all directories in /var/log and count files in each directories /var/log/*/* -> find all directories with a parent dir in /var/log and count files in each 
#   ##   directories /var/log -> count all files in /var/log and all of its subdirectories
#   directories = ["/var/cache/apt", "/tmp"]
#
#   ## Only count files that match the name pattern. Defaults to "*".
#   name = "*"
#
#   ## Count files in subdirectories. Defaults to true.
#   recursive = true
#
#   ## Only count regular files. Defaults to true.
#   regular_only = true
#
#   ## Follow all symlinks while walking the directory tree. Defaults to false.
#   follow_symlinks = false
#
#   ## Only count files that are at least this size. If size is a negative number, only count files that are smaller than the absolute value of size. Acceptable units are B, KiB, MiB, KB, ... 
#   ## Without quotes and units, interpreted as size in bytes.
#   size = "0B"
#
#   ## Only count files that have not been touched for at least this duration. If mtime is negative, only count files that have been touched in this duration. Defaults to "0s".
#   mtime = "0s"
# # Read stats about given file(s)
# [[inputs.filestat]]
#   ## Files to gather stats about. These accept standard unix glob matching rules, but with the addition of ** as a "super asterisk". See https://github.com/gobwas/glob.
#   files = ["/etc/telegraf/telegraf.conf", "/var/log/**.log"]
#
#   ## If true, read the entire file and calculate an md5 checksum.
#   md5 = false
# # Read real time temps from fireboard.io servers
# [[inputs.fireboard]]
#   ## Specify auth token for your account
#   auth_token = "invalidAuthToken"
#   ## You can override the fireboard server URL if necessary
#   # url = https://fireboard.io/api/v1/devices.json
#   ## You can set a different http_timeout if you need to You should set a string using an number and time indicator for example "12s" for 12 seconds.
#   # http_timeout = "4s"
# # Read metrics exposed by fluentd in_monitor plugin
# [[inputs.fluentd]]
#   ## This plugin reads information exposed by fluentd (using /api/plugins.json endpoint).
#   ##
#   ## Endpoint: - only one URI is allowed - https is not supported
#   endpoint = "http://localhost:24220/api/plugins.json"
#
#   ## Define which plugins have to be excluded (based on "type" field - e.g. monitor_agent)
#   exclude = [
#     "monitor_agent", "dummy", ]
# # Gather repository information from GitHub hosted repositories.
# [[inputs.github]]
#   ## List of repositories to monitor
#   repositories = [
#     "influxdata/telegraf", "influxdata/influxdb" ]
#
#   ## Github API access token.  Unauthenticated requests are limited to 60 per hour.
#   # access_token = ""
#
#   ## Github API enterprise url. Github Enterprise accounts must specify their base url.
#   # enterprise_base_url = ""
#
#   ## Timeout for HTTP requests.
#   # http_timeout = "5s"
#
#   ## List of additional fields to query. NOTE: Getting those fields might involve issuing additional API-calls, so please
#   ##       make sure you do not exceed the rate-limit of GitHub.
#   ##
#   ## Available fields are:
#   ##  - pull-requests -- number of open and closed pull requests (2 API-calls per repository)
#   # additional_fields = []
# # Gather metrics by iterating the files located on a Cloud Storage Bucket.
# [[inputs.google_cloud_storage]]
#   ## Required. Name of Cloud Storage bucket to ingest metrics from.
#   bucket = "my-bucket"
#
#   ## Optional. Prefix of Cloud Storage bucket keys to list metrics from.
#   # key_prefix = "my-bucket"
#
#   ## Key that will store the offsets in order to pick up where the ingestion was left.
#   offset_key = "offset_key"
#
#   ## Key that will store the offsets in order to pick up where the ingestion was left.
#   objects_per_iteration = 10
#
#   ## Required. Data format to consume. Each data format has its own unique set of configuration options. Read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"
#
#   ## Optional. Filepath for GCP credentials JSON file to authorize calls to Google Cloud Storage APIs. If not set explicitly, Telegraf will attempt to use Application Default Credentials, which is 
#   ## preferred.
#   # credentials_file = "path/to/my/creds.json"
# # Read flattened metrics from one or more GrayLog HTTP endpoints
# [[inputs.graylog]]
#   ## API endpoint, currently supported API:
#   ##
#   ##   - multiple (e.g. http://<host>:9000/api/system/metrics/multiple) - namespace (e.g. http://<host>:9000/api/system/metrics/namespace/{namespace})
#   ##
#   ## For namespace endpoint, the metrics array will be ignored for that call. Endpoint can contain namespace and multiple type calls.
#   ##
#   ## Please check http://[graylog-server-ip]:9000/api/api-browser for full list of endpoints
#   servers = [
#     "http://[graylog-server-ip]:9000/api/system/metrics/multiple", ]
#
#   ## Set timeout (default 5 seconds)
#   # timeout = "5s"
#
#   ## Metrics list List of metrics can be found on Graylog webservice documentation. Or by hitting the web service api at:
#   ##   http://[graylog-host]:9000/api/system/metrics
#   metrics = [
#     "jvm.cl.loaded", "jvm.memory.pools.Metaspace.committed" ]
#
#   ## Username and password
#   username = "" password = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Read metrics of HAProxy, via stats socket or http endpoints
# [[inputs.haproxy]]
#   ## List of stats endpoints. Metrics can be collected from both http and socket endpoints. Examples of valid endpoints:
#   ##   - http://myhaproxy.com:1936/haproxy?stats - https://myhaproxy.com:8000/stats - socket:/run/haproxy/admin.sock - /run/haproxy/*.sock - tcp://127.0.0.1:1936
#   ##
#   ## Server addresses not starting with 'http://', 'https://', 'tcp://' will be treated as possible sockets. When specifying local socket, glob patterns are supported.
#   servers = ["http://myhaproxy.com:1936/haproxy?stats"]
#
#   ## By default, some of the fields are renamed from what haproxy calls them. Setting this option to true results in the plugin keeping the original field names.
#   # keep_field_names = false
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Monitor disks' temperatures using hddtemp
# [[inputs.hddtemp]]
#   ## By default, telegraf gathers temps data from all disks detected by the hddtemp.
#   ##
#   ## Only collect temps from the selected disks.
#   ##
#   ## A * as the device name will return the temperature values of all disks.
#   ##
#   # address = "127.0.0.1:7634" devices = ["sda", "*"]
# # Read formatted metrics from one or more HTTP endpoints
# [[inputs.http]]
#   ## One or more URLs from which to read formatted metrics
#   urls = [
#     "http://localhost/metrics" ]
#
#   ## HTTP method
#   # method = "GET"
#
#   ## Optional HTTP headers
#   # headers = {"X-Special-Header" = "Special-Value"}
#
#   ## HTTP entity-body to send with POST/PUT requests.
#   # body = ""
#
#   ## HTTP Content-Encoding for write request body, can be set to "gzip" to compress body or "identity" to apply no encoding.
#   # content_encoding = "identity"
#
#   ## Optional file with Bearer token file content is added as an Authorization header
#   # bearer_token = "/path/to/file"
#
#   ## Optional HTTP Basic Auth Credentials
#   # username = "username" password = "pa$$word"
#
#   ## OAuth2 Client Credentials. The options 'client_id', 'client_secret', and 'token_url' are required to use OAuth2.
#   # client_id = "clientid" client_secret = "secret" token_url = "https://indentityprovider/oauth2/v1/token" scopes = ["urn:opc:idm:__myscopes__"]
#
#   ## HTTP Proxy support
#   # use_system_proxy = false http_proxy_url = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Minimal TLS version to accept by the client
#   # tls_min_version = "TLS12"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional Cookie authentication
#   # cookie_auth_url = "https://localhost/authMe" cookie_auth_method = "POST" cookie_auth_username = "username" cookie_auth_password = "pa$$word" cookie_auth_headers = { Content-Type = 
#   # "application/json", X-MY-HEADER = "hello" } cookie_auth_body = '{"username": "user", "password": "pa$$word", "authenticate": "me"}'
#   ## cookie_auth_renewal not set or set to "0" will auth once and never renew the cookie
#   # cookie_auth_renewal = "5m"
#
#   ## Amount of time allowed to complete the HTTP request
#   # timeout = "5s"
#
#   ## List of success status codes
#   # success_status_codes = [200]
#
#   ## Data format to consume. Each data format has its own unique set of configuration options, read more about them here: 
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   # data_format = "influx"
#
# # HTTP/HTTPS request given an address a method and a timeout
# [[inputs.http_response]]
#   ## List of urls to query.
#   # urls = ["http://localhost"]
#
#   ## Set http_proxy. Telegraf uses the system wide proxy settings if it's is not set.
#   # http_proxy = "http://localhost:8888"
#
#   ## Set response_timeout (default 5 seconds)
#   # response_timeout = "5s"
#
#   ## HTTP Request Method
#   # method = "GET"
#
#   ## Whether to follow redirects from the server (defaults to false)
#   # follow_redirects = false
#
#   ## Optional file with Bearer token file content is added as an Authorization header
#   # bearer_token = "/path/to/file"
#
#   ## Optional HTTP Basic Auth Credentials
#   # username = "username" password = "pa$$word"
#
#   ## Optional HTTP Request Body
#   # body = ''' {'fake':'data'} '''
#
#   ## Optional name of the field that will contain the body of the response. By default it is set to an empty String indicating that the body's content won't be added
#   # response_body_field = ''
#
#   ## Maximum allowed HTTP response body size in bytes. 0 means to use the default of 32MiB. If the response body size exceeds this limit a "body_read_error" will be raised.
#   # response_body_max_size = "32MiB"
#
#   ## Optional substring or regex match in body of the response (case sensitive)
#   # response_string_match = "\"service_status\": \"up\"" response_string_match = "ok" response_string_match = "\".*_status\".?:.?\"up\""
#
#   ## Expected response status code. The status code of the response is compared to this value. If they match, the field "response_status_code_match" will be 1, otherwise it will be 0. If the 
#   ## expected status code is 0, the check is disabled and the field won't be added.
#   # response_status_code = 0
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#   ## Use the given name as the SNI server name on each URL
#   # tls_server_name = ""
#   ## TLS renegotiation method, choose from "never", "once", "freely"
#   # tls_renegotiation_method = "never"
#
#   ## HTTP Request Headers (all values must be strings)
#   # [inputs.http_response.headers]
#   #   Host = "github.com"
#
#   ## Optional setting to map response http headers into tags If the http header is not present on the request, no corresponding tag will be added. If multiple instances of the http header are 
#   ## present, only the first value will be used.
#   # http_header_tags = {"HTTP_HEADER" = "TAG_NAME"}
#
#   ## Interface to use when dialing an address
#   # interface = "eth0"
#  ## DEPRECATED: The "httpjson" plugin is deprecated in version 1.6.0, use 'inputs.http' instead.
# # Read flattened metrics from one or more JSON HTTP endpoints
# [[inputs.httpjson]]
#   ## NOTE This plugin only reads numerical measurements, strings and booleans will be ignored.
#
#   ## Name for the service being polled.  Will be appended to the name of the measurement e.g. "httpjson_webserver_stats".
#   ##
#   ## Deprecated (1.3.0): Use name_override, name_suffix, name_prefix instead.
#   name = "webserver_stats"
#
#   ## URL of each server in the service's cluster
#   servers = [
#     "http://localhost:9999/stats/", "http://localhost:9998/stats/", ]
#   ## Set response_timeout (default 5 seconds)
#   response_timeout = "5s"
#
#   ## HTTP method to use: GET or POST (case-sensitive)
#   method = "GET"
#
#   ## Tags to extract from top-level of JSON server response.
#   # tag_keys = [
#   #   "my_tag_1", "my_tag_2" ]
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## HTTP Request Parameters (all values must be strings).  For "GET" requests, data will be included in the query.  For "POST" requests, data will be included in the request body as 
#   ## "x-www-form-urlencoded".
#   # [inputs.httpjson.parameters]
#   #   event_type = "cpu_spike" threshold = "0.75"
#
#   ## HTTP Request Headers (all values must be strings).
#   # [inputs.httpjson.headers]
#   #   X-Auth-Token = "my-xauth-token" apiVersion = "v1"
# # Gathers huge pages measurements. This plugin ONLY supports Linux
# [[inputs.hugepages]]
#   ## Supported huge page types:
#   ##   - "root" - based on root huge page control directory:
#   ##                  /sys/kernel/mm/hugepages - "per_node" - based on per NUMA node directories: /sys/devices/system/node/node[0-9]*/hugepages - "meminfo" - based on /proc/meminfo file
#   # types = ["root", "per_node"]
# # Gather Icinga2 status
# [[inputs.icinga2]]
#   ## Required Icinga2 server address
#   # server = "https://localhost:5665"
#
#   ## Collected Icinga2 objects ("services", "hosts") Specify at least one object to collect from /v1/objects endpoint.
#   # objects = ["services"]
#
#   ## Collect metrics from /v1/status endpoint Choose from:
#   ##     "ApiListener", "CIB", "IdoMysqlConnection", "IdoPgsqlConnection"
#   # status = []
#
#   ## Credentials for basic HTTP authentication
#   # username = "admin" password = "admin"
#
#   ## Maximum time to receive response.
#   # response_timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = true
# # Gets counters from all InfiniBand cards and ports installed This plugin ONLY supports Linux
# [[inputs.infiniband]]
#   # no configuration
# # Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints
# [[inputs.influxdb]]
#   ## Works with InfluxDB debug endpoints out of the box, but other services can use this format too. See the influxdb plugin's README for more details.
#
#   ## Multiple URLs from which to read InfluxDB-formatted JSON Default is "http://localhost:8086/debug/vars".
#   urls = [
#     "http://localhost:8086/debug/vars" ]
#
#   ## Username and password to send using HTTP Basic Authentication.
#   # username = "" password = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## http request & header timeout
#   timeout = "5s"
# ## Reads metrics from DPDK using v2 telemetry interface. This plugin ONLY supports Linux
# [[inputs.intel_dlb]]
#   ## Path to DPDK telemetry socket.
#   # socket_path = "/var/run/dpdk/rte/dpdk_telemetry.v2"
#
#   ## Default eventdev command list, it gathers metrics from socket by given commands. Supported options:
#   ##   "/eventdev/dev_xstats", "/eventdev/port_xstats", "/eventdev/queue_xstats", "/eventdev/queue_links"
#   # eventdev_commands = ["/eventdev/dev_xstats", "/eventdev/port_xstats", "/eventdev/queue_xstats", "/eventdev/queue_links"]
#
#   ## Detect DLB devices based on device id. Currently, only supported and tested device id is `0x2710`. Configuration added to support forward compatibility.
#   # dlb_device_types = ["0x2710"]
#
#   ## Specifies plugin behavior regarding unreachable socket (which might not have been initialized yet). Available choices:
#   ##   - error: Telegraf will return an error on startup if socket is unreachable - ignore: Telegraf will ignore error regarding unreachable socket on both startup and gather
#   # unreachable_socket_behavior = "error"
# # Intel PowerStat plugin enables monitoring of platform metrics (power, TDP) and per-CPU metrics like temperature, power and utilization. This plugin ONLY supports Linux
# [[inputs.intel_powerstat]]
#   ## The user can choose which package metrics are monitored by the plugin with the package_metrics setting: - The default, will collect "current_power_consumption",
#   ##   "current_dram_power_consumption" and "thermal_design_power" - Leaving this setting empty means no package metrics will be collected - Finally, a user can specify individual metrics to 
#   ## capture from the
#   ##   supported options list Supported options: "current_power_consumption", "current_dram_power_consumption", "thermal_design_power", "max_turbo_frequency", "uncore_frequency", 
#   ##   "cpu_base_frequency"
#   # package_metrics = ["current_power_consumption", "current_dram_power_consumption", "thermal_design_power"]
#
#   ## The user can choose which per-CPU metrics are monitored by the plugin in cpu_metrics array. Empty or missing array means no per-CPU specific metrics will be collected by the plugin. Supported 
#   ## options:
#   ##   "cpu_frequency", "cpu_c0_state_residency", "cpu_c1_state_residency", "cpu_c6_state_residency", "cpu_busy_cycles", "cpu_temperature", "cpu_busy_frequency" ATTENTION: cpu_busy_cycles is 
#   ## DEPRECATED - use cpu_c0_state_residency
#   # cpu_metrics = []
# # Collect statistics about itself
# [[inputs.internal]]
#   ## If true, collect telegraf memory stats.
#   # collect_memstats = true
# # Monitors internet speed using speedtest.net service
# [[inputs.internet_speed]]
#   ## This plugin downloads many MB of data each time it is run. As such consider setting a higher interval for this plugin to reduce the demand on your internet connection.
#   # interval = "60m"
#
#   ## Enable to reduce memory usage
#   # memory_saving_mode = false
#
#   ## Caches the closest server location
#   # cache = false
#
#   ## Server ID exclude filter Allows the user to exclude or include specific server IDs received by speedtest-go. Values in the exclude option will be skipped over. Values in the include option 
#   ## are the only options that will be picked from.
#   ##
#   ## See the list of servers speedtest-go will return at:
#   ##     https://www.speedtest.net/api/js/servers?engine=js&limit=10
#   ##
#   # server_id_exclude = [] server_id_include = []
# # This plugin gathers interrupts data from /proc/interrupts and /proc/softirqs.
# [[inputs.interrupts]]
#   ## When set to true, cpu metrics are tagged with the cpu.  Otherwise cpu is stored as a field.
#   ##
#   ## The default is false for backwards compatibility, and will be changed to true in a future version.  It is recommended to set to true on new deployments.
#   # cpu_as_tag = false
#
#   ## To filter which IRQs to collect, make use of tagpass / tagdrop, i.e.
#   # [inputs.interrupts.tagdrop]
#   #   irq = [ "NET_RX", "TASKLET" ]
# # Read metrics from the bare metal servers via IPMI
# [[inputs.ipmi_sensor]]
#   ## optionally specify the path to the ipmitool executable
#   # path = "/usr/bin/ipmitool"
#   ##
#   ## Setting 'use_sudo' to true will make use of sudo to run ipmitool. Sudo must be configured to allow the telegraf user to run ipmitool without a password.
#   # use_sudo = false
#   ##
#   ## optionally force session privilege level. Can be CALLBACK, USER, OPERATOR, ADMINISTRATOR
#   # privilege = "ADMINISTRATOR"
#   ##
#   ## optionally specify one or more servers via a url matching
#   ##  [username[:password]@][protocol[(address)]] e.g.
#   ##    root:passwd@lan(127.0.0.1)
#   ##
#   ## if no servers are specified, local machine sensor stats will be queried
#   ##
#   # servers = ["USERID:PASSW0RD@lan(192.168.1.1)"]
#
#   ## Recommended: use metric 'interval' that is a multiple of 'timeout' to avoid gaps or overlap in pulled data
#   interval = "30s"
#
#   ## Timeout for the ipmitool command to complete. Default is 20 seconds.
#   timeout = "20s"
#
#   ## Schema Version: (Optional, defaults to version 1)
#   metric_version = 2
#
#   ## Optionally provide the hex key for the IMPI connection.
#   # hex_key = ""
#
#   ## If ipmitool should use a cache for me ipmitool runs about 2 to 10 times faster with cache enabled on HP G10 servers (when using ubuntu20.04) the cache file may not work well for you if some 
#   ## sensors come up late
#   # use_cache = false
#
#   ## Path to the ipmitools cache file (defaults to OS temp dir) The provided path must exist and must be writable
#   # cache_path = ""
# # Gather packets and bytes counters from Linux ipsets
#   [[inputs.ipset]]
#     ## By default, we only show sets which have already matched at least 1 packet. set include_unmatched_sets = true to gather them all.
#     include_unmatched_sets = false
#     ## Adjust your sudo settings appropriately if using this option ("sudo ipset save") You can avoid using sudo or root, by setting appropriate privileges for the telegraf.service systemd 
#     ## service.
#     use_sudo = false
#     ## The default timeout of 1s for ipset execution can be overridden here:
#     # timeout = "1s"
#
# # Gather packets and bytes throughput from iptables This plugin ONLY supports Linux
# [[inputs.iptables]]
#   ## iptables require root access on most systems. Setting 'use_sudo' to true will make use of sudo to run iptables. Users must configure sudo to allow telegraf user to run iptables with no 
#   ## password. iptables can be restricted to only list command "iptables -nvL".
#   use_sudo = false
#   ## Setting 'use_lock' to true runs iptables with the "-w" option. Adjust your sudo settings appropriately if using this option ("iptables -w 5 -nvl")
#   use_lock = false
#   ## Define an alternate executable, such as "ip6tables". Default is "iptables".
#   # binary = "ip6tables"
#   ## defines the table to monitor:
#   table = "filter"
#   ## defines the chains to monitor. NOTE: iptables rules without a comment will not be monitored. Read the plugin documentation for more information.
#   chains = [ "INPUT" ]
# # Collect virtual and real server stats from Linux IPVS This plugin ONLY supports Linux
# [[inputs.ipvs]]
#   # no configuration
# # Read jobs and cluster metrics from Jenkins instances
# [[inputs.jenkins]]
#   ## The Jenkins URL in the format "schema://host:port"
#   url = "http://my-jenkins-instance:8080"
#   # username = "admin" password = "admin"
#
#   ## Set response_timeout
#   response_timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use SSL but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional Max Job Build Age filter Default 1 hour, ignore builds older than max_build_age
#   # max_build_age = "1h"
#
#   ## Optional Sub Job Depth filter Jenkins can have unlimited layer of sub jobs This config will limit the layers of pulling, default value 0 means unlimited pulling until no more sub jobs
#   # max_subjob_depth = 0
#
#   ## Optional Sub Job Per Layer In workflow-multibranch-plugin, each branch will be created as a sub job. This config will limit to call only the lasted branches in each layer, empty will use 
#   ## default value 10
#   # max_subjob_per_layer = 10
#
#   ## Jobs to include or exclude from gathering When using both lists, job_exclude has priority. Wildcards are supported: [ "jobA/*", "jobB/subjob1/*"]
#   # job_include = [ "*" ] job_exclude = [ ]
#
#   ## Nodes to include or exclude from gathering When using both lists, node_exclude has priority.
#   # node_include = [ "*" ] node_exclude = [ ]
#
#   ## Worker pool for jenkins plugin only Empty this field will use default value 5
#   # max_connections = 5
#  ## DEPRECATED: The "jolokia" plugin is deprecated in version 1.5.0, use 'inputs.jolokia2' instead.
# # Read JMX metrics through Jolokia
# [[inputs.jolokia]]
#   ## This is the context root used to compose the jolokia url NOTE that Jolokia requires a trailing slash at the end of the context root
#   context = "/jolokia/"
#
#   ## This specifies the mode used
#   # mode = "proxy"
#   #
#   ## When in proxy mode this section is used to specify further proxy address configurations. Remember to change host address to fit your environment.
#   # [inputs.jolokia.proxy]
#   #   host = "127.0.0.1" port = "8080"
#
#   ## Optional http timeouts
#   ##
#   ## response_header_timeout, if non-zero, specifies the amount of time to wait for a server's response headers after fully writing the request.
#   # response_header_timeout = "3s"
#   ##
#   ## client_timeout specifies a time limit for requests made by this client. Includes connection time, any redirects, and reading the response body.
#   # client_timeout = "4s"
#
#   ## List of servers exposing jolokia read service
#   [[inputs.jolokia.servers]]
#     name = "as-server-01" host = "127.0.0.1" port = "8080"
#     # username = "myuser" password = "mypassword"
#
#   ## List of metrics collected on above servers Each metric consists in a name, a jmx path and either a pass or drop slice attribute. Â This collect all heap memory usage metrics.
#   [[inputs.jolokia.metrics]]
#     name = "heap_memory_usage" mbean = "java.lang:type=Memory" attribute = "HeapMemoryUsage"
#
#   ##Â This collect thread counts metrics.
#   [[inputs.jolokia.metrics]]
#     name = "thread_count" mbean = "java.lang:type=Threading" attribute = "TotalStartedThreadCount,ThreadCount,DaemonThreadCount,PeakThreadCount"
#
#   ##Â This collect number of class loaded/unloaded counts metrics.
#   [[inputs.jolokia.metrics]]
#     name = "class_count" mbean = "java.lang:type=ClassLoading" attribute = "LoadedClassCount,UnloadedClassCount,TotalLoadedClassCount"
# # Read JMX metrics from a Jolokia REST agent endpoint
[[inputs.jolokia2_agent]]
#   # default_tag_prefix = "" default_field_prefix = "" default_field_separator = "."
#
#   # Add agents URLs to query
   urls = ["http://hello-jmx-agent:8888/jolokia"]
#   # username = "" password = "" response_timeout = "5s"
#
#   ## Optional origin URL to include as a header in the request. Some endpoints may reject an empty origin.
#   # origin = ""
#
#   ## Optional TLS config
#   # tls_ca = "/var/private/ca.pem" tls_cert = "/var/private/client.pem" tls_key = "/var/private/client-key.pem" insecure_skip_verify = false
#
#   ## Add metrics to read
    # ÐÐ»Ñ JVM Ð¼ÐµÑÑÐ¸Ðº ÑÐµÑÐµÐ· JMX
[[inputs.jolokia2_agent]]
  urls = ["http://hello-jmx-agent:8888/jolokia"]
  
  [[inputs.jolokia2_agent.metric]]
    name = "jvm_runtime"
    mbean = "java.lang:type=Runtime"
    paths = ["Uptime"]

  [[inputs.jolokia2_agent.metric]]
    name = "jvm_memory"
    mbean = "java.lang:type=Memory"
    paths = ["HeapMemoryUsage", "NonHeapMemoryUsage"]

  [[inputs.jolokia2_agent.metric]]
    name = "jvm_garbage_collector"
    mbean = "java.lang:type=GarbageCollector,name=*"
    paths = ["CollectionTime", "CollectionCount"]

  [[inputs.jolokia2_agent.metric]]
    name = "jvm_memory_pool"
    mbean = "java.lang:type=MemoryPool,name=*"
    paths = ["Usage"]
#    mbean = "java.lang:type=Memory"# # Read JMX metrics from a Jolokia REST proxy endpoint
#    paths = ["HeapMemoryUsage", "NonHeapMemoryUsage"]
# [[inputs.jolokia2_proxy]]
#   # default_tag_prefix = "" default_field_prefix = "" default_field_separator = "."
#
#   ## Proxy agent
#   url = "http://localhost:8080/jolokia"
#   # username = "" password = "" response_timeout = "5s"
#
#   ## Optional origin URL to include as a header in the request. Some endpoints may reject an empty origin.
#   # origin = ""
#
#   ## Optional TLS config
#   # tls_ca = "/var/private/ca.pem" tls_cert = "/var/private/client.pem" tls_key = "/var/private/client-key.pem" insecure_skip_verify = false
#
#   ## Add proxy targets to query
#   # default_target_username = "" default_target_password = ""
#   [[inputs.jolokia2_proxy.target]]
#     url = "service:jmx:rmi:///jndi/rmi://targethost:9999/jmxrmi"
#     # username = "" password = ""
#
#   ## Add metrics to read
#   [[inputs.jolokia2_proxy.metric]]
#     name = "java_runtime" mbean = "java.lang:type=Runtime" paths = ["Uptime"]
# # Read Kapacitor-formatted JSON metrics from one or more HTTP endpoints
# [[inputs.kapacitor]]
#   ## Multiple URLs from which to read Kapacitor-formatted JSON Default is "http://localhost:9092/kapacitor/v1/debug/vars".
#   urls = [
#     "http://localhost:9092/kapacitor/v1/debug/vars" ]
#
#   ## Time limit for http requests
#   timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Get kernel statistics from /proc/vmstat This plugin ONLY supports Linux
# [[inputs.kernel_vmstat]]
#   # no configuration
# # Read status information from one or more Kibana servers
# [[inputs.kibana]]
#   ## Specify a list of one or more Kibana servers
#   servers = ["http://localhost:5601"]
#
#   ## Timeout for HTTP requests
#   timeout = "5s"
#
#   ## HTTP Basic Auth credentials
#   # username = "username" password = "pa$$word"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Read metrics from the Kubernetes api
# [[inputs.kube_inventory]]
#   ## URL for the Kubernetes API. If empty in-cluster config with POD's service account token will be used.
#   # url = ""
#
#   ## Namespace to use. Set to "" to use all namespaces.
#   # namespace = "default"
#
#   ## Use bearer token for authorization. ('bearer_token' takes priority)
#   ##
#   ## Ignored if url is empty and in-cluster config is used.
#   ##
#   ## If both of these are empty, we'll use the default serviceaccount: at: /var/run/secrets/kubernetes.io/serviceaccount/token
#   ##
#   ## To auto-refresh the token, please use a file with the bearer_token option. If given a string, Telegraf cannot refresh the token periodically.
#   # bearer_token = "/var/run/secrets/kubernetes.io/serviceaccount/token"
#   ## OR deprecated in 1.24.0; use bearer_token with a file
#   # bearer_token_string = "abc_123"
#
#   ## Set response_timeout (default 5 seconds)
#   # response_timeout = "5s"
#
#   ## Optional Resources to exclude from gathering Leave them with blank with try to gather everything available. Values can be - "daemonsets", deployments", "endpoints", "ingress", "nodes", 
#   ## "persistentvolumes", "persistentvolumeclaims", "pods", "services", "statefulsets"
#   # resource_exclude = [ "deployments", "nodes", "statefulsets" ]
#
#   ## Optional Resources to include when gathering Overrides resource_exclude if both set.
#   # resource_include = [ "deployments", "nodes", "statefulsets" ]
#
#   ## selectors to include and exclude as tags.  Globs accepted. Note that an empty array for both will include all selectors as tags selector_exclude overrides selector_include if both set.
#   # selector_include = [] selector_exclude = ["*"]
#
#   ## Optional TLS Config Trusted root certificates for server
#   # tls_ca = "/path/to/cafile"
#   ## Used for TLS client certificate authentication
#   # tls_cert = "/path/to/certfile"
#   ## Used for TLS client certificate authentication
#   # tls_key = "/path/to/keyfile"
#   ## Send the specified TLS server name via SNI
#   # tls_server_name = "kubernetes.example.com"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Uncomment to remove deprecated metrics.
#   # fielddrop = ["terminated_reason"]
# # Read metrics from the kubernetes kubelet api
# [[inputs.kubernetes]]
#   ## URL for the kubelet, if empty read metrics from all nodes in the cluster
#   url = "http://127.0.0.1:10255"
#
#   ## Use bearer token for authorization. ('bearer_token' takes priority) If both of these are empty, we'll use the default serviceaccount: at: /var/run/secrets/kubernetes.io/serviceaccount/token
#   ##
#   ## To re-read the token at each interval, please use a file with the bearer_token option. If given a string, Telegraf will always use that token.
#   # bearer_token = "/var/run/secrets/kubernetes.io/serviceaccount/token"
#   ## OR
#   # bearer_token_string = "abc_123"
#
#   ## Pod labels to be added as tags.  An empty array for both include and exclude will include all labels.
#   # label_include = [] label_exclude = ["*"]
#
#   ## Set response_timeout (default 5 seconds)
#   # response_timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = /path/to/cafile tls_cert = /path/to/certfile tls_key = /path/to/keyfile
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
# # Read metrics from a LeoFS Server via SNMP
# [[inputs.leofs]]
#   ## An array of URLs of the form:
#   ##   host [ ":" port]
#   servers = ["127.0.0.1:4010"]
# # The libvirt plugin collects statistics from virtualized guests using virtualization libvirt API.
# [[inputs.libvirt]]
#      ## Domain names from which libvirt gather statistics. By default (empty or missing array) the plugin gather statistics from each domain registered in the host system.
#      # domains = []
#
#      ## Libvirt connection URI with hypervisor. The plugin supports multiple transport protocols and approaches which are configurable via the URI. The general URI form: 
#      ## driver[+transport]://[username@][hostname][:port]/[path][?extraparameters] Supported transport protocols: ssh, tcp, tls, unix URI examples for each type of transport protocol: 1. SSH: 
#      ## qemu+ssh://<USER@IP_OR_HOSTNAME>/system?keyfile=/<PATH_TO_PRIVATE_KEY>&known_hosts=/<PATH_TO_known_hosts> 2. TCP: qemu+tcp://<IP_OR_HOSTNAME>/system 3. TLS: 
#      ## qemu+tls://<HOSTNAME>/system?pkipath=/certs_dir/<COMMON_LOCATION_OF_CACERT_AND_SERVER_CLIENT_CERTS> 4. UNIX: qemu+unix:///system?socket=/<PATH_TO_libvirt-sock> Default URI is 
#      ## qemu:///system
#      # libvirt_uri = "qemu:///system"
#
#      ## Statistics groups for which libvirt plugin will gather statistics. Supported statistics groups: state, cpu_total, balloon, vcpu, interface, block, perf, iothread, memory, dirtyrate Empty 
#      ## array means no metrics for statistics groups will be exposed by the plugin. By default the plugin will gather all available statistics.
#      # statistics_groups = ["state", "cpu_total", "balloon", "vcpu", "interface", "block", "perf", "iothread", "memory", "dirtyrate"]
#
#      ## A list containing additional statistics to be exposed by libvirt plugin. Supported additional statistics: vcpu_mapping By default (empty or missing array) the plugin will not collect 
#      ## additional statistics.
#      # additional_statistics = []
#
# # Provides Linux CPU metrics This plugin ONLY supports Linux
# [[inputs.linux_cpu]]
#   ## Path for sysfs filesystem. See https://www.kernel.org/doc/Documentation/filesystems/sysfs.txt Defaults:
#   # host_sys = "/sys"
#
#   ## CPU metrics collected by the plugin. Supported options: "cpufreq", "thermal" Defaults:
#   # metrics = ["cpufreq"]
# # Provides Linux sysctl fs metrics
# [[inputs.linux_sysctl_fs]]
#   # no configuration
# # Read metrics exposed by Logstash
# [[inputs.logstash]]
#   ## The URL of the exposed Logstash API endpoint.
#   url = "http://127.0.0.1:9600"
#
#   ## Use Logstash 5 single pipeline API, set to true when monitoring Logstash 5.
#   # single_pipeline = false
#
#   ## Enable optional collection components.  Can contain "pipelines", "process", and "jvm".
#   # collect = ["pipelines", "process", "jvm"]
#
#   ## Timeout for HTTP requests.
#   # timeout = "5s"
#
#   ## Optional HTTP Basic Auth credentials.
#   # username = "username" password = "pa$$word"
#
#   ## Optional TLS Config.
#   # tls_ca = "/etc/telegraf/ca.pem" tls_cert = "/etc/telegraf/cert.pem" tls_key = "/etc/telegraf/key.pem"
#
#   ## Use TLS but skip chain & host verification.
#   # insecure_skip_verify = false
#
#   ## Optional HTTP headers.
#   # [inputs.logstash.headers]
#   #   "X-Special-Header" = "Special-Value"
# # Read metrics from local Lustre service on OST, MDS This plugin ONLY supports Linux
# [[inputs.lustre2]]
#   ## An array of /proc globs to search for Lustre stats If not specified, the default will work on Lustre 2.5.x

# Read metrics from one or many postgresql servers
[[inputs.postgresql]]
  ## Specify address via a url matching:
  ##   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]&statement_timeout=...
  ## or a simple string:
  ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production
  ## Users can pass the path to the socket as the host value to use a socket
  ## connection (e.g. `/var/run/postgresql`).
  ##
  ## All connection parameters are optional.
  ##
  ## Without the dbname parameter, the driver will default to a database
  ## with the same name as the user. This dbname is just for instantiating a
  ## connection with the server and doesn't restrict the databases we are trying
  ## to grab metrics for.
  ##
  address = "host=82.142.167.46 port=22022 user=postgres sslmode=disable"
  databases = ["public"]  
  ## A custom name for the database that will be used as the "server" tag in the
  ## measurement output. If not specified, a default one generated from
  ## the connection address is used.
  # outputaddress = "db01"

  ## connection configuration.
  ## maxlifetime - specify the maximum lifetime of a connection.
  ## default is forever (0s)
  ##
  ## Note that this does not interrupt queries, the lifetime will not be enforced
  ## whilst a query is running
  # max_lifetime = "0s"

  ## A  list of databases to explicitly ignore.  If not specified, metrics for all
  ## databases are gathered.  Do NOT use with the 'databases' option.
  # ignored_databases = ["postgres", "template0", "template1"]

  ## A list of databases to pull metrics about. If not specified, metrics for all
  ## databases are gathered.  Do NOT use with the 'ignored_databases' option.
  # databases = ["app_production", "testing"]

  ## Whether to use prepared statements when connecting to the database.
  ## This should be set to false when connecting through a PgBouncer instance
  ## with pool_mode set to transaction.
  prepared_statements = true

# # Gather metrics about network interfaces
 [[inputs.net]]
#   ## By default, telegraf gathers stats from any up interface (excluding loopback)
#   ## Setting interfaces will tell it to gather these explicit interfaces,
#   ## regardless of status. When specifying an interface, glob-style
#   ## patterns are also supported.
#   ##
    interfaces = ["eth*", "enp0s[0-1]", "lo"]
#   ##
#   ## On linux systems telegraf also collects protocol stats.
#   ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.
#   ##
#   # ignore_protocol_stats = false
#   ##

# # Read TCP metrics such as established, time wait and sockets counts.
[[inputs.netstat]]

[[inputs.prometheus]]
  urls = ["http://hello-jmx-agent:8888/metrics"]
  name_override = "hello_jvm"
  interval = "30s"

